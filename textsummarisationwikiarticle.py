# -*- coding: utf-8 -*-
"""TextSummarisationWikiArticle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16SE6ua4FPGo0ojee-WfXx0qWW_6S5ZDV
"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <h1><marquee style='width: 80%; color: blue;'><b>Text Summarisation </b></marquee>
#  
# <marquee style='width: 80%; color: red;'><b><i>Wikipedia Article</i></b></marquee></h1>

! pip install transformers

"""##  Summarisation using transfromer library pipeline
  Default distilbart model


"""

from transformers import pipeline
summarizer = pipeline("summarization")
summary_text_transformers_bart= summarizer(ai_blog_wiki)

summary_text_transformers_bart[0]['summary_text']

"""## Summarization with XLM Transformers"""

# Importing model and tokenizer
from transformers import XLMWithLMHeadModel, XLMTokenizer
# Instantiating the model and tokenizer 
tokenizer=XLMTokenizer.from_pretrained('xlm-mlm-en-2048')
model=XLMWithLMHeadModel.from_pretrained('xlm-mlm-en-2048')

# encoding the input text
input_ids=tokenizer.encode(ai_blog_wiki[0:2500], return_tensors='pt', truncation=True)

summary_ids = model.generate(input_ids, early_stopping=True)

# Decode and print the summary
XLM_summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)
print(XLM_summary)

"""## Summarization with GPT-2 Transformers"""

from transformers import GPT2Tokenizer,GPT2LMHeadModel

# Instantiating the model and tokenizer with gpt-2
tokenizer=GPT2Tokenizer.from_pretrained('gpt2')
model=GPT2LMHeadModel.from_pretrained('gpt2')

# Encoding text to get input ids & pass them to model.generate()
inputs=tokenizer(ai_blog_wiki,return_tensors='pt',max_length=512)
summary_gpt2=model.generate(inputs['input_ids'],early_stopping=True)

summary_gpt2=model.generate(inputs['input_ids'],early_stopping=True)
gpt2_text_summary= tokenizer.batch_decode(summary_gpt2)

gpt2_text_summary

"""# Text Summarization using Gensim with TextRank"""

# Importing package and summarizer
import gensim
from gensim.summarization import summarize

"""# Text Rank Algorithm

*   TextRank - extractive summarization technique 
*   Words which occur more frequently are significant
* Sentences containing highly frequent words are important 
* assigns scores to each sentence in the text 
The top-ranked sentences make it to the summary.
"""

# Passing the text corpus to summarizer 
short_summary_genensim = summarize(ai_blog_wiki)
print(short_summary_genensim)

# Importing package and summarizer
import gensim
from gensim.summarization import summarize

# Summarization when both ratio & word count is given
summary_gensim=summarize(ai_blog_wiki, ratio=0.1)

print(summary_gensim)

"""# LexRank Summarizer

* A sentence which is similar to many other sentences of the text has a high probability of being important. 
* The approach of LexRank is that a particular sentence is recommended by other similar sentences and hence is ranked higher.
* Higher the rank, higher is the priority of being included in the summarized text.
"""

# Installing and Importing sumy
! pip install sumy
import sumy

# Importing the parser and tokenizer
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer

# Import the LexRank summarizer
from sumy.summarizers.lex_rank import LexRankSummarizer

# Initializing the parser
import nltk
nltk.download('punkt')
my_parser = PlaintextParser.from_string(ai_blog_wiki,Tokenizer('english'))

# Import the LexRank summarizer
from sumy.summarizers.lex_rank import LexRankSummarizer

# Creating a summary of 3 sentences.
lex_rank_summarizer = LexRankSummarizer()
lexrank_summary = lex_rank_summarizer(my_parser.document,sentences_count=3)

# Printing the summary
for sentence in lexrank_summary:
  print(sentence)

lexrank_summary

"""# LSA (Latent semantic analysis)
* Unsupervised learning algorithm that can be used for extractive text summarization.

* Extracts semantically significant sentences by applying singular value decomposition(SVD) to the matrix of term-document frequency. 
"""

# Import the summarizer
from sumy.summarizers.lsa import LsaSummarizer

# Parsing the text string using PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser
parser=PlaintextParser.from_string(ai_blog_wiki,Tokenizer('english'))

# creating the summarizer
lsa_summarizer=LsaSummarizer()
lsa_summary= lsa_summarizer(parser.document,3)

# Printing the summary
for sentence in lsa_summary:
    print(sentence)

lsa_summary

"""# Abstractive Summarisation with t5 transformers"""

! pip install sentencepiece

# Importing requirements
from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration

# Instantiating the model and tokenizer 
my_model = T5ForConditionalGeneration.from_pretrained('t5-small')
tokenizer = T5Tokenizer.from_pretrained('t5-small')

# Concatenating the word "summarize:" to raw text
text = "summarize:" + ai_blog_wiki
text

text

# encoding the input text
input_ids=tokenizer.encode(text, return_tensors='pt', truncation=True)

summary_ids = model.generate(input_ids, max_length=150, min_length=80, length_penalty=5., num_beams=2)

# Decoding and printing the summary
t5_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(t5_summary)

"""# Summarisation with BART transformers"""

# Importing the model
from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig

# Loading the model and tokenizer for bart-large-cnn

tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

# Encoding the inputs and passing them to model.generate()
inputs = tokenizer.batch_encode_plus([ai_blog_wiki],return_tensors='pt')
summary_ids = model.generate(inputs['input_ids'], early_stopping=True)

# Decoding and printing the summary
bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(bart_summary)