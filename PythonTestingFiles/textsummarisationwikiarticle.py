# -*- coding: utf-8 -*-
"""TextSummarisationWikiArticle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16SE6ua4FPGo0ojee-WfXx0qWW_6S5ZDV
"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <h1><marquee style='width: 80%; color: blue;'><b>Text Summarisation </b></marquee>
#  
# <marquee style='width: 80%; color: red;'><b><i>Wikipedia Article</i></b></marquee></h1>

! pip install transformers

ai_blog_wiki = """
Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by humans or animals. Leading AI textbooks define the field as the study of "intelligent agents": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.[a] Some popular accounts use the term "artificial intelligence" to describe machines that mimic "cognitive" functions that humans associate with the human mind, such as "learning" and "problem solving", however this definition is rejected by major AI researchers.[b]

AI applications include advanced web search engines (i.e. Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri or Alexa), self-driving cars (e.g. Tesla), and competing at the highest level in strategic game systems (such as chess and Go),[2] As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of AI, a phenomenon known as the AI effect.[3] For instance, optical character recognition is frequently excluded from things considered to be AI,[4] having become a routine technology.[5]

Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an "AI winter"),[8][9] followed by new approaches, success and renewed funding.[7][10] AI research has tried and discarded many different approaches during its lifetime, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.[11][10]

The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects.[c] General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.[12] To solve these problems, AI researchers use versions of search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.

The field was founded on the assumption that human intelligence "can be so precisely described that a machine can be made to simulate it". This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction and philosophy since antiquity.[13] Science fiction and futurology have also suggested that, with its enormous potential and power, AI may become an existential risk to humanity.
"""

"""##  Summarisation using transfromer library pipeline
  Default distilbart model


"""

from transformers import pipeline
summarizer = pipeline("summarization")
summary_text_transformers_bart= summarizer(ai_blog_wiki)

summary_text_transformers_bart[0]['summary_text']

"""## Summarization with XLM Transformers"""

# Importing model and tokenizer
from transformers import XLMWithLMHeadModel, XLMTokenizer
# Instantiating the model and tokenizer 
tokenizer=XLMTokenizer.from_pretrained('xlm-mlm-en-2048')
model=XLMWithLMHeadModel.from_pretrained('xlm-mlm-en-2048')

# encoding the input text
input_ids=tokenizer.encode(ai_blog_wiki[0:2500], return_tensors='pt', truncation=True)

summary_ids = model.generate(input_ids, early_stopping=True)

# Decode and print the summary
XLM_summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)
print(XLM_summary)

"""## Summarization with GPT-2 Transformers"""

from transformers import GPT2Tokenizer,GPT2LMHeadModel

# Instantiating the model and tokenizer with gpt-2
tokenizer=GPT2Tokenizer.from_pretrained('gpt2')
model=GPT2LMHeadModel.from_pretrained('gpt2')

# Encoding text to get input ids & pass them to model.generate()
inputs=tokenizer(ai_blog_wiki,return_tensors='pt',max_length=512)
summary_gpt2=model.generate(inputs['input_ids'],early_stopping=True)

summary_gpt2=model.generate(inputs['input_ids'],early_stopping=True)
gpt2_text_summary= tokenizer.batch_decode(summary_gpt2)

gpt2_text_summary

"""# Text Summarization using Gensim with TextRank"""

# Importing package and summarizer
import gensim
from gensim.summarization import summarize

"""# Text Rank Algorithm

*   TextRank - extractive summarization technique 
*   Words which occur more frequently are significant
* Sentences containing highly frequent words are important 
* assigns scores to each sentence in the text 
The top-ranked sentences make it to the summary.
"""

# Passing the text corpus to summarizer 
short_summary_genensim = summarize(ai_blog_wiki)
print(short_summary_genensim)

# Importing package and summarizer
import gensim
from gensim.summarization import summarize

# Summarization when both ratio & word count is given
summary_gensim=summarize(ai_blog_wiki, ratio=0.1)

print(summary_gensim)

"""# LexRank Summarizer

* A sentence which is similar to many other sentences of the text has a high probability of being important. 
* The approach of LexRank is that a particular sentence is recommended by other similar sentences and hence is ranked higher.
* Higher the rank, higher is the priority of being included in the summarized text.
"""

# Installing and Importing sumy
! pip install sumy
import sumy

# Importing the parser and tokenizer
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer

# Import the LexRank summarizer
from sumy.summarizers.lex_rank import LexRankSummarizer

# Initializing the parser
import nltk
nltk.download('punkt')
my_parser = PlaintextParser.from_string(ai_blog_wiki,Tokenizer('english'))

# Import the LexRank summarizer
from sumy.summarizers.lex_rank import LexRankSummarizer

# Creating a summary of 3 sentences.
lex_rank_summarizer = LexRankSummarizer()
lexrank_summary = lex_rank_summarizer(my_parser.document,sentences_count=3)

# Printing the summary
for sentence in lexrank_summary:
  print(sentence)

lexrank_summary

"""# LSA (Latent semantic analysis)
* Unsupervised learning algorithm that can be used for extractive text summarization.

* Extracts semantically significant sentences by applying singular value decomposition(SVD) to the matrix of term-document frequency. 
"""

# Import the summarizer
from sumy.summarizers.lsa import LsaSummarizer

# Parsing the text string using PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser
parser=PlaintextParser.from_string(ai_blog_wiki,Tokenizer('english'))

# creating the summarizer
lsa_summarizer=LsaSummarizer()
lsa_summary= lsa_summarizer(parser.document,3)

# Printing the summary
for sentence in lsa_summary:
    print(sentence)

lsa_summary

"""# Abstractive Summarisation with t5 transformers"""

! pip install sentencepiece

# Importing requirements
from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration

# Instantiating the model and tokenizer 
my_model = T5ForConditionalGeneration.from_pretrained('t5-small')
tokenizer = T5Tokenizer.from_pretrained('t5-small')

# Concatenating the word "summarize:" to raw text
text = "summarize:" + ai_blog_wiki
text

text

# encoding the input text
input_ids=tokenizer.encode(text, return_tensors='pt', truncation=True)

summary_ids = model.generate(input_ids, max_length=150, min_length=80, length_penalty=5., num_beams=2)

# Decoding and printing the summary
t5_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(t5_summary)

"""# Summarisation with BART transformers"""

# Importing the model
from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig

# Loading the model and tokenizer for bart-large-cnn

tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

# Encoding the inputs and passing them to model.generate()
inputs = tokenizer.batch_encode_plus([ai_blog_wiki],return_tensors='pt')
summary_ids = model.generate(inputs['input_ids'], early_stopping=True)

# Decoding and printing the summary
bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(bart_summary)