{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Latex_walker_aggeregation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MQoCYiXmby6a"
      },
      "outputs": [],
      "source": [
        "directory_name = \"/content/drive/MyDrive/thesis\"\n",
        "latext_directory_name = \"/content/drive/MyDrive/latex_papers/2001.06776\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_latex_file_path = \"/content/drive/MyDrive/latex_papers/2001.06776/Krishnamurthy20.tex\""
      ],
      "metadata": {
        "id": "eBb6yUr8dcU3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def latex_file_read_remove_comment(file_path):\n",
        "  latex_text_list = open(file_path).readlines()\n",
        "  latex_file_wo_comments = []\n",
        "    # removal of comments in the latex files\n",
        "  for line in latex_text_list:\n",
        "    if not line.startswith(\"%\"):\n",
        "      latex_file_wo_comments.append(line)\n",
        "  latex_text_wo_comments = \"\".join(latex_file_wo_comments)\n",
        "  return latex_text_wo_comments"
      ],
      "metadata": {
        "id": "ZzVE-dBVZl4u"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_latex_text_wo_comments = latex_file_read_remove_comment(main_latex_file_path)"
      ],
      "metadata": {
        "id": "wsit_eNAbdTu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def latex_extract_input_files(latex_text_cleaned):\n",
        "    input_file_names = re.findall(r'\\\\input{(.*?)}', latex_text_cleaned, re.S)\n",
        "    return input_file_names"
      ],
      "metadata": {
        "id": "k_ZsmowefrXX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imported_latex_file_names= latex_extract_input_files(main_latex_text_wo_comments)"
      ],
      "metadata": {
        "id": "RcES8E1Qf44o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imported_latex_file_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DIBQ7CAgTRY",
        "outputId": "86d8deda-c426-432a-f13a-a7b2980b9531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['01-intro', '02-complex', '03-moments', '05-appendix']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latext_directory_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "j3_150oUdb4Y",
        "outputId": "5f0c1abb-cd4c-42ec-eb07-209550222575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/latex_papers/2001.06776'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "input_files_path_list = []\n",
        "input_file_path_content_dir = {}\n",
        "for latex_input_files in imported_latex_file_names:\n",
        "    file_path = os.path.join(latext_directory_name, latex_input_files) + \".tex\"\n",
        "    input_files_path_list.append(file_path)\n",
        "    latex_text_wo_comments= latex_file_read_remove_comment(file_path)\n",
        "    input_file_command= \"\\input{\" + latex_input_files +  \"}\"  \n",
        "    input_file_path_content_dir[input_file_command]  = latex_text_wo_comments"
      ],
      "metadata": {
        "id": "VOYLoktpmyqJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def create_file_path_content_dir(imported_latex_file_names):\n",
        "  input_files_path_list = []\n",
        "  input_file_path_content_dir = {}\n",
        "  for latex_input_files in imported_latex_file_names:\n",
        "      file_path = os.path.join(latext_directory_name, latex_input_files) + \".tex\"\n",
        "      input_files_path_list.append(file_path)\n",
        "      latex_text_wo_comments= latex_file_read_remove_comment(file_path)\n",
        "      input_file_command= \"\\input{\" + latex_input_files +  \"}\"  \n",
        "      input_file_path_content_dir[input_file_command]  = latex_text_wo_comments\n",
        "  return input_file_path_content_dir"
      ],
      "metadata": {
        "id": "wLfMBQ93lxok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path_content_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yi6F1AlvHPa",
        "outputId": "37962c1b-403a-4787-f609-57138b22d26b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\\\input{01-intro}': '\\\\section{Introduction}\\nMixture modeling is a powerful method in the statistical toolkit, with\\nwidespread use across the\\nsciences~\\\\citep{titterington1985statistical}. Starting with the\\nseminal work of~\\\\cite{dasgupta1999learning}, computational and\\nstatistical aspects of learning mixture models have been the subject\\nof intense investigation in the theoretical computer science and\\nstatistics\\ncommunities~\\\\citep{achlioptas2005spectral,kalai2010efficiently,belkin2010polynomial,arora2001learning,moitra2010settling,feldman2008learning,chan2014efficient,acharya2017sample,hopkins2018mixture,diakonikolas2018list,kothari2018robust,hardt2015tight}.\\n\\nIn this literature, there are two flavors of result: (1)\\n\\\\emph{parameter estimation}, where the goal is to identify the mixing\\nweights and the parameters of each component from samples, and (2)\\n\\\\emph{density estimation} or PAC-learning, where the goal is simply to\\nfind a distribution that is close in some distance (e.g., TV distance)\\nto the data-generating mechanism. Density estimation can be further\\nsubdivided into \\\\emph{proper} and \\\\emph{improper learning} approaches\\ndepending on whether the algorithm outputs a distribution from the\\ngiven mixture family or not. These three guarantees are quite\\ndifferent. Apart from Gaussian mixtures, where all types of results\\nexist, prior work for other mixture families largely focuses on\\ndensity estimation, and very little is known for parameter estimation\\noutside of Gaussian mixture models.  In this paper, we focus on\\nparameter estimation and provide two new approaches, both of which\\napply to several mixture families.\\n\\nOur first approach is analytic in nature and yields new sample\\ncomplexity guarantees for univariate mixture models including\\nGaussian, Binomial, and Poisson. Our key technical insight is that we\\ncan relate the total variation between two candidate mixtures to a\\ncertain Littlewood polynomial, and then use complex analytic\\ntechniques to establish separation in TV-distance. With this\\nseparation result, we can use density estimation techniques\\n(specifically proper learning techniques) to find a candidate mixture\\nthat is close in TV-distance to the data generating mechanism.  The\\nresults we obtain via this approach are labeled as ``analytic\" in\\nTable~\\\\ref{tab:tab1}. This approach has recently led to important\\nadvances in the trace reconstruction and population recovery problems;\\nsee work by \\\\cite{DeOS17}, \\\\cite{NazarovP17}, and \\\\cite{DeOS17x}.\\n\\n\\n\\n\\nOur second approach is based on the \\\\emph{method of moments}, a\\npopular approach for learning Gaussian mixtures, and is more\\nalgebraic.  Roughly, these algorithms are based on expressing moments\\nof the mixture model as polynomials of the component parameters, and\\nthen solving a polynomial system using estimated moments. This approach\\nhas been studied in some generality by~\\\\citet{belkin2010polynomial}\\nwho show that it can succeed for a large class of mixture\\nmodels. However, as their method uses non-constructive arguments from\\nalgebraic geometry it cannot be used to bound how many moments are\\nrequired, which is essential in determining the sample complexity; see\\na discussion in \\\\cite[Section 7.6]{moitra2018algorithmic}. In\\ncontrast, our approach does yield bounds on how many moments suffice\\nand can be seen as a quantified version of the results\\nin~\\\\citet{belkin2010polynomial}. The results we obtain via this\\napproach are labeled as ``algebraic\\'\\' in Table~\\\\ref{tab:tab1}.\\n\\nThe literature on mixture models is quite large, and we have just referred to a sample of most relevant papers here. A bigger overview on learning distributions can be found in the recent monographs such as \\\\cite{moitra2018algorithmic,diakonikolas2016learning}.\\n\\n\\n\\n\\\\subsection{Overview of results}\\nAs mentioned, an overview of our sample complexity results are\\ndisplayed in Table~\\\\ref{tab:tab1}, where in all cases we consider a\\nuniform mixture of $k$ distributions. Our guarantees are for\\n\\\\emph{exact} parameter estimation, under the assumption that the\\nmixture parameters are discretized to a particular resolution, given\\nin the third column of the table. Theorem statements are given in the\\nsequel.\\n\\nAt first glance the guarantees seem weak, since they all involve\\nexponential dependence in problem parameters. However, except for the\\nGaussian case, these results are the first guarantees for parameter\\nestimation for these distributions. All prior results we are aware of\\nconsider density\\nestimation~\\\\citep{chan2013learning,feldman2008learning}.\\n\\nFor the mixtures of discrete distributions, such as binomial and negative binomial with shared trial parameter, or Poisson/geometric/chi-squared mixtures with certain discretizations, it seems like the dependence of sample complexity on the number of components $k$ is polynomial (see Table~\\\\ref{tab:tab1}). Note that for these examples $k \\\\le N$, the upper bounds on parameter values. Therefore the actual dependence on $k$ can still be interpreted as exponential. The results are especially interesting when $k$ is large and possibly growing with $N$.\\n\\nFor Gaussian mixtures, the most interesting aspect of our bound is the\\npolynomial dependence on the number of components $k$ (first row of\\nTable~\\\\ref{tab:tab1}).  In our setting and taking $\\\\sigma = 1$, the\\nresult of~\\\\citet{moitra2010settling} is applicable, and it yields\\n$\\\\epsilon^{-O(k)}$ sample complexity, which is incomparable to our\\n$k^3\\\\exp(O(\\\\epsilon^{-2/3}))$ bound. Note that our result avoids an\\nexponential dependence in $k$, trading this off for an exponential\\ndependence on the discretization/accuracy parameter\\n$\\\\epsilon$.\\\\footnote{Due to our discretization structure, our results\\n  do not contradict the lower bounds\\n  of~\\\\citet{moitra2010settling,hardt2015tight}.} Other results for\\nGaussian mixtures either 1) consider density\\nestimation~\\\\citep{daskalakis2014faster,feldman2008learning}, which is\\nqualitatively quite different from parameter estimation, 2) treat $k$\\nas constant~\\\\citep{hardt2015tight,kalai2010efficiently}, or 3) focus\\non the high dimensional setting and require separation assumptions\\n(see for example~\\\\cite{diakonikolas2017statistical}\\nand~\\\\cite{moitra2018algorithmic}).\\n\\nAs such, our results\\n  reflect a new sample complexity tradeoff for parameter estimation\\nin Gaussian mixtures.\\n\\n\\n\\n\\nAs another note, using ideas from~\\\\citep{NazarovP17,DeOS17}, one can show that the analytic result for Binomial mixtures is optimal. \\nThis\\nraises the question of whether the other results are also optimal or\\nis learning a Binomial mixture intrinsically harder than learning,\\ne.g., a Poisson or Gaussian mixture?\\n\\nAs a final remark, our assumption that parameters are discretized is\\nrelated to separation conditions that appear in the literature on\\nlearning Gaussian mixtures. However, our approach does not seem to\\nyield guarantees when the parameters do not exactly fall into the\\ndiscretization. We hope to resolve this shortcoming in future work.\\n\\n\\n\\n\\n\\n\\n\\\\begin{table}\\n\\\\begin{center}\\n\\\\renewcommand{\\\\arraystretch}{2}\\n\\\\begin{tabular}{|c | c | c | c | c |}\\n\\\\hline\\nDistribution & Pdf/Pmf $f(x;\\\\theta)$ & Discretization & Sample Complexity & Approach\\\\\\\\\\n\\\\hline\\\\hline\\nGaussian & $\\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}e^{-\\\\frac{(x-\\\\mu)^{2}}{\\\\sigma^2}}$ & $\\\\mu_i \\\\in \\\\epsilon\\\\ZZ$ & $k^3 \\\\exp(O((\\\\sigma/\\\\epsilon)^{2/3}))$ & Analytic\\\\\\\\\\n\\\\hline\\n\\\\multirow{ 2}{*}{Binomial} & \\\\multirow{ 2}{*}{${n \\\\choose x}p^{x}(1-p)^{n-x}$} & $n_i \\\\in \\\\{1,2,\\\\ldots,N\\\\}$ & $\\\\exp(O(((N/p)^{1/3})))$ & Analytic \\\\protect\\\\footnotemark\\\\\\\\\\n \\\\cline{3-5} \\n& & $p_i \\\\in  \\\\{0,\\\\epsilon,\\\\ldots,1\\\\}$ & $O(k^2(n/\\\\epsilon)^{8/\\\\sqrt{\\\\epsilon}})$ & Algebraic \\\\\\\\\\n\\\\hline\\nPoisson & $\\\\frac{\\\\lambda^x e^{-\\\\lambda}}{x!}$ & $\\\\lambda_i \\\\in \\\\{0,1,\\\\dots,N\\\\}$ & $\\\\exp(O(N^{1/3})) $ & Analytic\\\\\\\\ \\\\hline\\n\\\\multirow{ 2}{*}{Geometric} & \\\\multirow{ 2}{*}{$(1-p)^x p $} & $1/p_i \\\\in \\\\{1,\\\\dots,N\\\\}$ & $O(k^2(\\\\sqrt{N})^{8\\\\sqrt{N}})$  & Algebraic\\\\\\\\  \\\\cline{3-5} \\n& & $p_i \\\\in \\\\{0,\\\\epsilon,\\\\dots,1\\\\}$ & $O(\\\\frac{k^2}{\\\\epsilon^{8/\\\\sqrt{\\\\epsilon}+2}}\\\\log \\\\frac{1}{{\\\\epsilon}})$  & Algebraic\\\\\\\\ \\\\hline\\n$\\\\chi^2$ & $\\\\frac{x^{n/2-1}e^{-x/2}}{2^{n/2}\\\\Gamma(n/2)}$ & $n_i \\\\in \\\\{0,1, \\\\dots,N\\\\}$ &  $\\\\exp(O(N^{1/3})) $ & Analytic \\\\\\\\ \\\\hline\\nNegative Binomial & ${x+r-1 \\\\choose x} (1-p)^rp^x$ & $r_i \\\\in  \\\\{1,2,\\\\ldots, N\\\\}$ & $\\\\exp(O((N/p)^{1/3})) $ & Analytic\\\\\\\\ \\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Overview of our results. Results are given for uniform mixtures of $k$ different components but some can be extended to non-uniform mixtures.\\\\label{tab:tab1} Note that for rows 2, 4, 7, and 8, $k$ does not appear. This is because $k\\\\leq N$ and other terms dominate.}\\n\\\\end{table}\\n\\n\\n\\\\subsection{Our techniques}\\nTo establish these results, we take two loosely related approaches.\\nIn our analytic approach, the key structural result is to lower bound\\nthe total variation distance between two mixtures $\\\\Mc,\\\\Mc\\'$ by a\\ncertain Littlewood polynomial. For each distribution type, if the\\nparameter is $\\\\theta$, we find a function $G_t: \\\\RR \\\\to \\\\CC$ such that\\n\\\\begin{align*}\\n\\\\EE[G_t(X)] = \\\\exp(it\\\\theta).\\n\\\\end{align*}\\n(For Gaussians, $G_t$ is essentially the characteristic function).\\nSuch functions can be used to obtain Littlewood polynomials from the\\ndifference in expectation for two different mixtures, for example if\\nthe parameters $\\\\theta$ are integral and the mixture weights are\\nuniform. Applying complex analytic results on Littlewood polynomials,\\nthis characterization yields a lower bound on the total variation\\ndistance between mixtures, at which point we may use density\\nestimation techniques for parameter learning. Specifically we use the\\nminimum distance estimator (see,~\\\\citet[Sec.~6.8]{devroye2012combinatorial}), which\\nis based on the idea of Scheffe sets. Scheffe sets are building blocks of the Scheffe estimator, \\ncommonly used in density estimation, e.g.~\\\\citet{suresh2014near}. \\n\\n\\n\\\\footnotetext{We obtained this result as a byproduct of sparse trace reconstruction~\\\\citep{krishnamurthy2019trace}. In fact, the present work was motivated by the observation that the  technique we were using there is much more general.}\\n\\n  %submission at a different venue, in a entirely different context. In\\n  %fact, the present work was motivated by the observation that the\\n  %technique we developed there is much more general.}\\n  \\n\\n\\n\\nOur algebraic approach is based on the more classical method of\\nmoments. Our key innovation here is a combinatorial argument to bound\\nthe number of moments that we need to estimate in order to exactly\\nidentify the correct mixture parameters. In more detail, when the\\nparameters belong to a discrete set, we show that the moments reveal\\nvarious statistics about the multi-set of parameters in the\\nmixture. Then, we adapt and extend classical combinatorics results on\\nsequence reconstruction to argue that two distinct multi-sets must\\ndisagree on a low-order moment. These combinatorial results are\\nrelated to the Prouhet-Tarry-Escott problem (see, e.g.,\\n\\\\cite{Borwein2002}) which also has connections to Littlewood\\npolynomials. To wrap up we use standard concentration arguments to\\nestimate all the necessary moments, which yields the sample complexity\\nguarantees.\\n\\nWe note that the complex analytic technique provides non-trivial result only for those mixtures for which an appropriate function $G_t$ exists. On the other hand, the algebraic approach works for all mixtures whose $\\\\ell^{th}$ moment can be described as a polynomial of degree exactly $\\\\ell$ in its unknown parameters. In \\\\cite{belkin2010polynomial}, it was shown that most distributions have this later property. In general, where both methods can be applied, the complex analytic techniques typically provide tighter sample complexity bounds than the algebraic ones.\\n\\n\\n',\n",
              " '\\\\input{02-complex}': \"\\\\section{Learning Mixtures via Characteristic Functions}\\nIn this section, we show how analysis of the characteristic function\\ncan yield sample complexity guarantees for learning mixtures. At a\\nhigh level, the recipe we adopt is the following.\\n\\\\begin{enumerate}\\n\\\\item First, we show that, in a general sense, the total variation distance between two\\n  separated mixtures is lower bounded by the $L_{\\\\infty}$ norm of\\n  their characteristic functions.\\n\\\\item Next, we use complex analytic methods and specialized arguments\\n  for each particular distribution to lower bound the latter norm.\\n\\\\item Finally, we use the minimum distance\\n  estimator~\\\\citep{devroye2012combinatorial} to find a mixture that is\\n  close in total variation to the data generating distribution. Using\\n  uniform convergence arguments this yields exact parameter learning.\\n\\\\end{enumerate}\\n\\nThe two main results we prove in this section are listed below.\\n\\\\begin{theorem}[Learning Gaussian mixtures]{\\\\label{thm:main}}\\nLet $\\\\mathcal{M}=\\\\frac{1}{k}\\\\sum_{i=1}^k\\\\mathcal{N}(\\\\mu_i,\\\\sigma^2)$\\nbe a uniform mixture of $k$ univariate Gaussians, with known shared\\ncovariance $\\\\sigma^2$ and with distinct means $\\\\mu_i \\\\in\\n\\\\epsilon\\\\mathbb{Z}$. Then there exists an algorithm that requires $k^3\\n\\\\exp(O((\\\\sigma/\\\\epsilon)^{2/3}))$ samples from $\\\\mathcal{M}$ and\\nexactly identifies the parameters $\\\\{\\\\mu_i\\\\}_{i=1}^k$ with high\\nprobability.\\n\\\\end{theorem}\\n\\n\\\\begin{theorem}[Learning Poisson mixtures]\\nLet $\\\\mathcal{M} = \\\\frac{1}{k}\\\\sum_{i=1}^k\\\\textrm{Poi}(\\\\lambda_i)$\\nwhere $\\\\lambda_i \\\\in \\\\{0,1,\\\\ldots,N\\\\}$ for each $i$ are distinct. Then there exists\\nan algorithm that that requires $\\\\exp(O(N^{1/3})))$ samples %\\\\andrew{shouldn't this be $n^{1/3}$?}\\nfrom $\\\\mathcal{M}$ to exactly identify the parameters\\n$\\\\{\\\\lambda_i\\\\}_{i=1}^k$ with high probability.\\n\\\\end{theorem}\\n\\nThere are some technical differences in deriving the results for\\nGaussian vs Poisson mixtures.  Namely, because of finite choice of\\nparameters we can take a union bound over the all possible incorrect\\nmixtures for the latter case, which is not possible for Gaussian. For\\nGaussian mixtures we instead use an approach based on VC dimension.\\nThe results for negative binomial mixtures and chi-squared mixtures\\n(shown in Table~\\\\ref{tab:tab1}) follow the same route as the Poisson\\nmixture.  As reported in Table~\\\\ref{tab:tab1}, this approach also\\nyields results for mixtures of binomial distributions that we obtained in a different context in our prior work~\\\\citep{krishnamurthy2019trace}.\\n\\n\\n\\n\\n\\\\subsection{Total Variation and Characteristic Functions}\\nLet $\\\\{f_\\\\theta\\\\}_{\\\\theta \\\\in \\\\Theta}$ denote a parameterized family\\nof distributions over a sample space $\\\\Omega \\\\subset \\\\mathbb{R}$,\\nwhere $f_\\\\theta$ denotes either a pdf or pmf, depending on the\\ncontext.  We call $\\\\Mc$ a (finite) \\\\emph{$\\\\Theta$-mixture} if $\\\\Mc$\\nhas pdf/pmf $\\\\sum_{\\\\theta \\\\in \\\\Ac}\\\\alpha_\\\\theta f_\\\\theta$ and $\\\\Ac\\n\\\\subset \\\\Theta, |\\\\Ac| =k$. For a distribution with density $f$ (we use\\ndistribution and density interchangeably in the sequel), define the\\n\\\\emph{characteristic function} $C_f(t) \\\\equiv \\\\mathbb{E}_{X \\\\sim\\n  f}[e^{itX}]$. For any two distribution $f,f'$ defined over a sample\\nspace $\\\\Omega \\\\subseteq \\\\mathbb{R}$ the variational distance (or the\\nTV-distance) is defined to be\\n$\\\\variation{f - f'} \\\\equiv \\\\frac{1}{2}\\\\int_{\\\\Omega}\\n\\\\left|\\\\frac{df'}{df} - 1\\\\right| df$.\\nFor a function $G:\\\\Omega \\\\to \\\\CC$ define the $L_{\\\\infty}$ norm to be\\n$\\\\|G\\\\|_{\\\\infty} = \\\\sup_{\\\\omega \\\\in \\\\Omega} |G(\\\\omega)|$ where\\n$|\\\\cdot|$ denotes the modulus.\\n\\n\\nAs a first step, our aim is to show that the total variation distance\\nbetween $\\\\Mc=\\\\sum_{\\\\theta \\\\in \\\\Ac}\\\\alpha_\\\\theta f_\\\\theta$ and any\\nother mixture $\\\\Mc'$ given by $\\\\sum_{\\\\theta \\\\in \\\\Bc}\\\\beta_\\\\theta\\nf_\\\\theta, \\\\Bc \\\\subset \\\\Theta, |\\\\Bc| =k$ is lower bounded. The\\nfollowing elementary lemma completes the first step of the outlined\\napproach.\\n\\n\\\\begin{lemma}\\\\label{lem:chartv}\\nFor any two distributions $f,f'$ defined over the same sample space $\\\\Omega\\\\subseteq\\\\mathbb{R}$, we have\\n\\\\begin{align*}\\n\\\\variation{f -f'} \\\\ge \\\\frac{1}{2} \\\\sup_{t \\\\in \\\\reals}|C_f(t) -C_{f'}(t)|.\\n\\\\end{align*}\\nMore generally, for any $G: \\\\Omega \\\\to \\\\CC$ and $\\\\Omega' \\\\subset \\\\Omega$ we have\\n\\\\begin{align*}\\n\\\\variation{f -f'} \\\\geq \\\\left(2\\\\sup_{x \\\\in \\\\Omega'}|G(x)|\\\\right)^{-1} \\\\Big(\\\\left| \\\\EE_{X \\\\sim f} G(X) - \\\\EE_{X \\\\sim f'}G(X) \\\\right| \\\\\\\\\\\\qquad \\\\,\\\\,- \\\\int_{x \\\\in\\\\Omega \\\\setminus \\\\Omega'}|G(x)| \\\\cdot |df(x) - df'(x)|\\\\Big).\\n\\\\end{align*}\\n\\\\end{lemma}\\n\\\\begin{proof}\\nWe prove the latter statement, which implies the former since for the\\nfunction $G(x) = e^{itx}$ we have $\\\\sup_{x} |G(x)| = 1$. %By the\\nWe have\\n\\\\begin{align*}\\n|\\\\EE_{X \\\\sim f} G(X) &- \\\\EE_{X \\\\sim f'}G(X)| \\\\leq \\\\int_{x \\\\in \\\\Omega}|G(x)| \\\\cdot |df(x) - df'(x)| \\\\\\\\\\n& \\\\le 2\\\\sup_{x \\\\in \\\\Omega'}|G(x)| \\\\cdot \\\\variation{f -f'} + \\\\int_{x \\\\in\\\\Omega \\\\setminus \\\\Omega'}|G(x)| \\\\cdot |df(x) - df'(x)|. \\\\tag*\\\\qedhere\\n\\\\end{align*}\\n\\\\end{proof}\\n\\nEquipped with the lower bound in Lemma~\\\\ref{lem:chartv}, for each type\\nof distribution, we set out to find a good function $G$ to witness\\nseparation in total variation distance. As we will see shortly, for a\\nparametric family $f_\\\\theta$, it will be convenient to find a family\\nof functions $G_t$ such that\\n\\\\begin{align*}\\n\\\\EE_{X \\\\sim f_\\\\theta}[G_t(X)] = \\\\exp(it\\\\theta).\\n\\\\end{align*}\\nOf course, to apply Lemma~\\\\ref{lem:chartv}, it will also be important\\nto understand $\\\\|G_t\\\\|_{\\\\infty}$. While such functions are specific to\\nthe parametric model in question, the remaining analysis will be\\nunified. We derive such functions and collect the relevant properties\\nin the following lemma. At a high level, the calculations are based on\\nreverse engineering from the characteristic function, e.g., finding a\\nchoice $t'(t)$ such that $C_f(t') = \\\\exp(it\\\\theta)$.\\n\\n\\\\begin{lemma}\\\\label{lem:list}\\nLet $z=\\\\exp(i t)$ where $t \\\\in [-\\\\pi/L,\\\\pi /L]$. \\n\\\\begin{itemize}\\n\\\\item { Gaussian}. If $X\\\\sim \\\\Nc(\\\\mu,\\\\sigma)$ and $G_t(x) = e^{itx}$ then \\n\\\\[\\\\EE[G_t(X)]=\\\\exp(-\\\\sigma^2 t^2/2)z^\\\\mu \\\\mbox{ and } \\\\|G_t\\\\|_{\\\\infty}= 1 \\\\ . \\\\]\\n\\\\item { Poisson}. If $X\\\\sim \\\\textrm{Poi}(\\\\lambda)$ and $G_t(x)=(1+it)^x$ then \\n\\\\[\\\\EE[G_t(X)]=z^\\\\lambda \\\\mbox{ and } |G_t(x)| \\\\leq (1+t^2)^{x/2} \\\\ . \\\\]\\n\\\\item { Chi-Squared}. If $X\\\\sim \\\\chi^2(\\\\ell)$ and $G_t(x)=\\\\exp(x/2-xe^{-2it}/2)$  then \\n\\\\[ \\\\EE[G_t(X)]=z^\\\\ell \\\\mbox{ and } |G_t(x)|\\\\leq e^{cxt^{2}+O(xt^{4})} \\\\ .\\\\]\\n\\\\item { Negative Binomial}. If $X\\\\sim \\\\textrm{NB}(r,p)$ and $G_t(x)=\\\\left({1}/{p}-({1}/{p}-1)e^{-it}\\\\right)^x$ then \\n\\\\[\\\\EE[G_t(X)]=z^r \\\\mbox{ and } |G_t(x)|\\\\leq e^{-cx\\\\frac{(1-p)t^2}{p^2}} \\\\ .\\\\]\\n\\\\end{itemize}\\n\\\\end{lemma}\\n\\\\begin{proof}\\nHere we give the argument for Poisson distributions only. The\\nremaining calculations are deferred to the appendix.  For Poisson\\nrandom variables, if $G_t(x) = (1+it)^x$ then since $|1+it|^2 = 1 +\\nt^2$ the second claim follows. For the first:\\n\\\\begin{align*}\\n\\\\EE[G_t(X)]=\\\\exp(\\\\lambda((1+it)-1))=z^\\\\lambda.\\\\tag*\\\\qedhere\\n\\\\end{align*}\\n\\\\end{proof}\\n\\n\\n\\n\\n\\\\subsection{Variational Distance Between Mixtures}\\nWe crucially use the following lemma. % that also appear in \\\\cite{NazarovP17}, \\n\\\\begin{lemma}[\\\\cite{BorweinE97}] \\\\label{lem:npb} Let $a_0, a_1, a_2, \\\\dots \\\\in \\\\{0,1,-1\\\\}$ be such that not all of them are zero. For any complex number $z$, let $A(z) \\\\equiv \\\\sum_k a_k z^k.$ Then, for some absolute constant $c$, \\n$$\\n\\\\max_{-\\\\pi/L \\\\le t \\\\le \\\\pi/L} |A(e^{it})| \\\\ge e^{-cL} \\\\ . \\n$$\\n\\\\end{lemma}\\n\\nWe will also need the following `tail bound' lemma.\\n\\\\begin{lemma}\\\\label{lem:tail}\\nSuppose $a>1$ is any real number  and $r \\\\in \\\\reals_+$. \\nFor any discrete random variable $X$ with support $\\\\integers$ and pmf $f$,\\n$$\\n\\\\sum_{x\\\\ge r} a^x f(x) \\\\le \\\\frac{\\\\EE[a^{2X}]}{a^{r-1}}.\\n$$\\n\\\\end{lemma}\\n\\\\begin{proof}\\nNote that, $\\\\Pr(X \\\\ge x) =  \\\\Pr(a^{2X-2x} \\\\ge 1) \\\\le \\\\EE[a^{2X-2x}]$. We have,\\n\\\\begin{align*}\\n\\\\sum_{x\\\\ge r} a^x \\\\Pr(X =x) &\\\\le \\\\sum_{x\\\\ge r} a^x \\\\Pr(X \\\\ge x)\\n\\\\le  \\\\sum_{x\\\\ge r} a^x \\\\EE[a^{2X-2x}] =  \\\\EE[a^{2X}] \\\\sum_{x\\\\ge r} a^{-x} \\\\le \\\\frac{\\\\EE[a^{2X}]}{a^{r-1}}.\\\\tag*\\\\qedhere\\n\\\\end{align*}\\n\\\\end{proof}\\n\\n\\\\begin{theorem}[TV Lower Bounds] \\\\label{thm:tv} The following bounds hold on distance between two different mixtures assuming all $k$ parameters are distinct for each mixture.\\n\\\\begin{itemize}[leftmargin=10pt]\\n\\\\item \\\\emph{Gaussian:} $\\\\mathcal{M} =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\Nc(\\\\mu_i,\\\\sigma)$\\nand \\n$\\\\mathcal{M}' =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\Nc(\\\\mu_i',\\\\sigma)$\\nwhere \\n$\\\\mu_i, \\\\mu_i' \\\\in \\\\epsilon \\\\ZZ$.\\nThen\\n\\\\[\\n\\\\variation{\\\\mathcal{M}' -\\\\mathcal{M}} \\\\geq k^{-1} \\\\exp(-\\\\Omega((\\\\sigma/\\\\epsilon)^{2/3})) \\\\ .\\n\\\\]\\n\\\\item \\\\emph{Poisson:} $\\\\mathcal{M} =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\textrm{Poi}(\\\\lambda_i)$\\nand \\n$\\\\mathcal{M}' =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\textrm{Poi}(\\\\lambda_i')$\\nwhere \\n$\\\\lambda_i, \\\\lambda_i' \\\\in \\\\{0, 1,\\\\ldots, N\\\\}$.\\nThen \\\\[\\\\variation{\\\\mathcal{M}' -\\\\mathcal{M}} \\\\geq k^{-1} \\\\exp(-\\\\Omega(N^{1/3})) \\\\ .\\\\]\\n\\\\item \\\\emph{Chi-Squared:}  $\\\\mathcal{M} =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\chi^2(\\\\ell_i)$\\nand \\n$\\\\mathcal{M}' =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\chi^2(\\\\ell_i')$\\nwhere  $\\\\ell_i, \\\\ell_i' \\\\in \\\\{ 1, 2, \\\\ldots, N\\\\}$. \\nThen \\\\[ \\\\variation{\\\\mathcal{M}' -\\\\mathcal{M}} \\\\geq k^{-1} \\\\exp(- \\\\Omega(N^{1/3})) \\\\ .\\\\] \\n\\\\item \\\\emph{Negative Binomial:} \\n$\\\\mathcal{M} =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\textrm{NB}(r_i,p)$\\nand \\n$\\\\mathcal{M}' =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\textrm{NB}(r_i',p)$\\nwhere $r_i, r_i' \\\\in \\\\{1, 2, \\\\ldots , N \\\\}$. Then\\n\\\\[\\\\variation{\\\\mathcal{M}' -\\\\mathcal{M}} \\\\geq  k^{-1} \\\\exp(- \\\\Omega((N/p)^{1/3})) \\\\ .\\\\] \\n\\n\\n\\\\end{itemize}\\n\\\\end{theorem}\\n\\\\begin{proof}\\nAs above we give the argument for Poisson random variables, deferring the others to the appendix. \\nLet $X \\\\sim \\\\Mc$ and $X' \\\\sim \\\\Mc'$. Then, for $w=1+it$, from Lemma~\\\\ref{lem:list},\\n$$\\n\\\\EE(w^X) - \\\\EE(w^{X'}) = \\\\frac1k\\\\sum_{j=1}^k (e^{it \\\\lambda_j} - e^{it \\\\lambda'_j}) .\\n$$\\n\\n\\n\\n\\n\\n\\n\\n\\nNow we use  \\n Lemma~\\\\ref{lem:chartv} with $G(x) = w^x$, $\\\\Omega' = \\\\{0,1, \\\\dots, 2N\\\\}$ and $t \\\\le 1$, to have, \\n\\\\begin{align*}\\n| \\\\EE(w^X) - \\\\EE(w^{X'})| &= \\\\Big|\\\\sum_x (w^x \\\\Mc(x) - w^x \\\\Mc'(x)) \\\\Big|\\n \\\\le \\\\sum_x |w|^x  |\\\\Mc(x) - \\\\Mc'(x)| \\\\\\\\\\n& \\\\le  (1+t^2)^{2N} \\\\sum_x  |\\\\Mc(x) - \\\\Mc'(x)| + \\\\sum_{x> 4N} (1+t^2)^{x/2}e^{-N}\\\\frac{N^x}{x!}\\\\\\\\\\n&\\\\le (1+t^2)^{2N} \\\\sum_x  |\\\\Mc(x) - \\\\Mc'(x)| + \\\\sum_{x> 4N} 2^{x/2}e^{-N}\\\\frac{N^x}{x!}.\\n \\\\end{align*}\\nNow using Lemma \\\\ref{lem:tail}, \\n\\\\begin{align*}\\n| \\\\EE(w^X) - \\\\EE(w^{X'})|& \\\\le  2(1+t^2)^{2N} \\\\variation{\\\\Mc -\\\\Mc'} + \\\\frac{\\\\EE[2^X]}{2^{2N-1/2}}\\n\\\\le 2e^{2t^2N} \\\\variation{\\\\Mc -\\\\Mc'} + \\\\frac{e^{N} }{2^{2N-1/2}}\\\\\\\\\\n& =2e^{2\\\\pi^2N/L^2} \\\\variation{\\\\Mc -\\\\Mc'} + \\\\exp(-\\\\Omega(N)),\\n \\\\end{align*}\\n by taking  $|t| \\\\le \\\\frac{\\\\pi}{L}$. % and where we have used Chernoff inequality to bound the tail probability of a Poisson distribution.\\nNow using Lemma~\\\\ref{lem:npb}, there exist an absolute constant $c$ such that,\\n\\\\begin{align*}\\n\\\\max_{-\\\\frac{\\\\pi}{ L}\\\\le t \\\\le \\\\frac{\\\\pi}{ L}} \\\\big| \\\\sum_{j=1}^k (e^{it \\\\lambda_j} - e^{it \\\\lambda'_j})\\\\big| \\\\ge e^{-cL}.\\n\\\\end{align*}\\nTherefore by setting $L = N^{1/3}$,\\n\\\\begin{align*}\\n \\\\variation{\\\\Mc -\\\\Mc'} \\\\ge  (2k)^{-1} e^{-cL-2\\\\pi^2N /L^2} - \\\\exp(-\\\\Omega(N)) \\\\ge  k^{-1}\\\\exp(- \\\\Omega(N^{1/3})).\\\\tag*\\\\qedhere\\n\\\\end{align*}\\n\\n\\n\\n\\n\\\\end{proof}\\n \\n \\n\\n \\\\subsection{Parameter Learning}\\\\label{sec:imp}\\n\\n \\\\paragraph{Union Bound Approach for Discrete Distributions}\\n We begin with the following proposition which follows from Theorem 7.1 of~\\\\citet{devroye2012combinatorial}.\\n \\\\begin{lemma}\\\\label{lem:uni}\\n Suppose $F= \\\\{f_\\\\nu\\\\}_{\\\\nu \\\\in \\\\Theta}$ is a class of distribution such that for any $\\\\nu,\\\\nu' \\\\in \\\\Theta$, $\\\\variation{f_\\\\nu-f_{\\\\nu'}} \\\\ge \\\\delta$. Then  $O(\\\\log |\\\\Theta|/\\\\delta^2)$ samples from a distribution $f$ in $F$ suffice to distinguish it from all other distributions in $F$ with high probability.\\n \\\\end{lemma}\\n\\n\\nFor the mixture of Poissons, $\\\\mathcal{M} =\\n\\\\frac{1}{k}\\\\sum_{i=1}^k \\\\textrm{Poi}(\\\\lambda_i)$ where $\\\\lambda_i \\\\in\\n\\\\{0, 1,\\\\ldots, N\\\\},$ the number of choices for parameters in the\\nmixture is $(N+1)^k$. Now using Lemmas~\\\\ref{thm:tv} and \\\\ref{lem:uni},\\n$\\\\exp(O(N^{1/3}))$ samples are sufficient to learn the\\nparameters of the mixture.\\n\\n\\n\\n\\nExactly the same argument applies to\\nmixtures of Chi-Squared and Negative-Binomial\\ndistributions, yielding $\\\\exp(O(N^{1/3}))$ and $\\n\\\\exp(O((N/p)^{1/3}))$ samples suffice, respectively. However, for Gaussians we need a more intricate approach. \\n\\n\\n \\\\paragraph{VC Approach for Gaussians} To learn the parameters of a\\n Gaussian mixture\\n \\\\[\\\\mathcal{M} =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\Nc(\\\\mu_i,\\\\sigma) ~~\\\\mbox{ where }~~\\\\mu_i \\\\in \\\\{\\\\ldots, -2\\\\epsilon, -\\\\epsilon, 0,\\\\epsilon, 2\\\\epsilon \\\\ldots \\\\} \\\\]\\n  we use the minimum distance estimator precisely defined in \\\\cite[Section~6.8]{devroye2012combinatorial}. Let $\\\\Ac \\\\equiv \\\\{\\\\{x: \\\\Mc(x) \\\\ge \\\\Mc'(x)\\\\}: \\\\text{ for any two mixtures } \\\\Mc \\\\ne \\\\Mc'\\\\}$ be a collection of subsets. Let $P_m$ denote the empirical probability measure induced by the $m$ samples. Then, choose a mixture $\\\\hat{\\\\Mc}$ for which \\n  the quantity $\\\\sup_{A\\\\in \\\\Ac}  |\\\\Pr_{\\\\sim\\\\hat{\\\\Mc}}(A) - P_m(A) |$ is minimum (or within $1/m$ of the infimum). This is the minimum distance estimator, whose performance is guaranteed by the following proposition~\\\\cite[Thm.~6.4]{devroye2012combinatorial}.\\n  \\n  \\n  \\n  \\n  \\\\begin{proposition}%[\\\\cite[Thm.~6.4]{devroye2012combinatorial}]\\n % Using Theorem~6.4 of \\\\cite{devroye2012combinatorial}, we can write:\\n    Given $m$ samples from $\\\\Mc$ and with $\\\\Delta = \\\\sup_{A \\\\in \\\\Ac}|\\\\Pr_{\\\\sim\\\\Mc}(A) - P_m(A)|$, we have\\n  $$\\n  \\\\variation{\\\\hat{\\\\Mc} -\\\\Mc} \\\\le 4\\\\Delta +\\\\frac{3}{m}.\\n  $$\\n\\\\end{proposition}\\nWe now upper bound the right-hand side of the above inequality.  Via\\nMcDiarmid's inequality and a standard symmetrization argument, $\\\\Delta$ is concentrated around its mean which is a function of $VC(\\\\Ac)$, the VC dimension of the class $\\\\Ac$, see\\n\\\\cite[Section~4.3]{devroye2012combinatorial}:  % we have\\n$$\\n\\\\variation{\\\\hat{\\\\Mc} - \\\\Mc} \\\\leq 4\\\\Delta + O(1/m) \\\\leq 4\\\\avg_{\\\\sim\\\\Mc} \\\\Delta + O(1/\\\\sqrt{m}) \\\\le c \\\\sqrt{\\\\frac{VC(\\\\Ac)}{m}},\\n$$ with high probability, for an absolute constant $c$.  This latter term is bounded by the following. \\n\\n \\n\\\\begin{lemma}\\nFor the class $\\\\Ac$ defined above, the VC dimension is given by $VC(\\\\Ac) = O(k)$.\\n\\\\end{lemma}\\n\\\\begin{proof}\\nFirst of all we show that any element of the set $\\\\Ac$ can be written\\nas union of at most $4k-1$ intervals in $\\\\reals$. For this we use the\\nfact that a linear combination of $k$ Gaussian pdfs $f(x) =\\n\\\\sum_{i=1}^{k} \\\\alpha_{i} f_{i}(x)$ where $f_i$s normal pdf\\n$\\\\Nc(\\\\mu_i,\\\\sigma^2_i)$ and $\\\\alpha_i \\\\in \\\\reals, 1\\\\le i\\\\le k$ has at\\nmost $2k-2$ zero-crossings \\\\citep{kalai2012disentangling}. Therefore,\\nfor any two mixtures of interest $\\\\Mc(x) -\\\\Mc'(x)$ has at most $4k-2$\\nzero-crossings. Therefore any $A\\\\in \\\\Ac$ must be a union of at most\\n$4k-1$ contiguous regions in $\\\\reals$.  It is now an easy exercise to\\nsee that the VC dimension of such a class is $\\\\Theta(k)$.\\n\\\\end{proof}\\n\\n\\n\\nAs a result the error of the minimum distance estimator is $O(\\\\sqrt{k/m})$ with high probability. \\nBut from Theorem~\\\\ref{thm:tv}, notice that for any other mixture $\\\\Mc'$ we must have,\\n$$\\n\\\\variation{\\\\Mc -\\\\Mc'} \\\\ge  k^{-1} \\\\exp(-\\\\Omega((\\\\sigma/\\\\epsilon)^{2/3})).\\n$$ \\nAs long as $\\\\variation{\\\\hat{\\\\Mc} -\\\\Mc} \\\\le \\\\frac12 \\\\variation{\\\\Mc -\\\\Mc'}$ we will exactly identify the parameters. Therefore \\n$m = k^3 \\\\exp(O((\\\\sigma/\\\\epsilon)^{2/3}))$ samples suffice to exactly learn the parameters with high probability.\\n\\n\\n\\n\\n\\\\subsection{Extension to Non-Uniform Mixtures}\\nThe above results extend to non-uniform mixtures, where the main change is that we require a generalization of Lemma~\\\\ref{lem:npb}. The result, also proved by \\\\cite{BorweinE97}, states that if $a_0, a_1, a_2, \\\\ldots \\\\in [-1, 1]$ with $\\\\textrm{poly}(n)$ precision then $\\\\max_{-\\\\pi/L \\\\le \\\\theta \\\\le \\\\pi/L} |A(e^{i\\\\theta})| \\\\ge e^{-cL \\\\log n}$, for an absolute constant $c$. This weaker bound yields an extra $\\\\textup{poly}(n)$ factor in the sample complexity.  %\\\\andrew{maybe add something more here?}\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
              " '\\\\input{03-moments}': \"\\\\section{Learning Mixtures via Moments}\\n\\nThere are some mixtures where the problem of learning parameters is not amenable to the approach in the previous section. A simple motivating example is learning the parameters $p_i\\\\in \\\\{0, \\\\epsilon, 2\\\\epsilon, 3\\\\epsilon, \\\\ldots, 1\\\\}$ values\\\\footnote{Note that we are implicitly assuming $1/\\\\epsilon$ is integral here and henceforth.} in the mixture $\\\\mathcal{M}=\\\\frac{1}{k} \\\\sum_{i=1}^k \\\\textup{Bin}(n,p_i)$. In this section, we present an alternative procedure for learning such mixtures. The basic idea is as follows:\\n\\\\begin{itemize}\\n\\\\item We compute moments $\\\\avg X^\\\\ell$ exactly for $\\\\ell=0,1,\\\\ldots, T$ by taking sufficiently many samples. The number of samples will depend on $T$ and the precision of the parameters of the mixture.\\n\\\\item We argue that if $T$ is sufficiently large, then these moments uniquely define the parameters of the mixture. To do this we use a combinatorial result due to \\\\cite{KRASIKOV1997344}.\\n\\\\end{itemize}\\n\\nIn this section, it will be convenient to define a function $m_\\\\ell$ on multi-sets where \\n\\\\[m_\\\\ell (A):=\\\\sum_{a\\\\in A} a^\\\\ell  \\\\ .\\\\]\\n\\nOur main result is as follows:\\n\\n\\\\begin{theorem}[Learning Binomial mixtures]\\nLet $\\\\mathcal{M}=\\\\frac{1}{k}\\\\sum_{i=1}^k\\\\textrm{Bin}(n,p_i)$\\nbe a uniform mixture of $k$ binomials, with known shared\\nnumber of trials $n$ and unknown probabilities $p_1, \\\\ldots, p_k \\\\in \\\\{0,\\\\epsilon,2\\\\epsilon, \\\\ldots, 1\\\\}$. Then, provided $n \\\\ge 4/\\\\sqrt{\\\\epsilon}$, the first $4/\\\\sqrt{\\\\epsilon}$ moments suffice to learn the parameters $p_i$ and  there exists an algorithm that, when given\\n$O(k^2 (n/\\\\epsilon)^{8/\\\\sqrt{\\\\epsilon}})$ samples from\\n$\\\\mathcal{M}$, exactly identifies the parameters $\\\\{p_i\\\\}_{i=1}^k$\\nwith high probability.\\n\\\\end{theorem}\\n\\n\\n\\n\\\\paragraph{Computing the Moments}\\nWe compute the $\\\\ell$th moment as $S_{\\\\ell,t}=\\\\sum Y_i^\\\\ell /t$ where $Y_1, \\\\ldots, Y_t\\\\sim X$. % and let \\\\[S_\\\\ell=\\\\sum Y_i^\\\\ell /t \\\\ . \\\\]\\n\\\\begin{lemma}\\\\label{lem:boundmoments}\\n$\\\\Pr[|S_{\\\\ell,t}-\\\\avg X^\\\\ell|\\\\geq \\\\gamma] \\\\leq \\\\frac{\\\\avg X^{2\\\\ell}}{t\\\\gamma^2} \\\\leq \\\\frac{(2\\\\ell)! } {\\\\gamma^2 t} \\\\inf_\\\\alpha \\\\left ( \\\\frac{\\\\avg e^{\\\\alpha X}}{\\\\alpha^{2\\\\ell}} \\\\right )$ where the last inequality assumes the all the moments of $X$ are non-negative. \\n\\\\end{lemma}\\n\\\\begin{proof}\\nBy the Chebyshev bound, \\n\\\\[\\\\Pr[|S_{\\\\ell,t}-\\\\avg X^\\\\ell|\\\\geq \\\\gamma] \\\\leq \\\\frac{Var(S_{\\\\ell,t})}{\\\\gamma^2}=\\\\frac{Var(X^\\\\ell)}{t\\\\gamma^2}\\n\\\\leq\\n\\\\frac{\\\\avg X^{2\\\\ell}}{t\\\\gamma^2}.\\n\\\\]\\nWe then use the moment generating function:\\n for all $\\\\alpha>0$, $\\\\avg X^{2\\\\ell}\\\\leq (2\\\\ell)! \\\\avg e^{\\\\alpha X}/\\\\alpha^{2\\\\ell}$.\\n\\\\end{proof}\\n\\nThe following corollary, tailors the above lemma for a mixture of binomial distributions.\\n\\\\begin{corollary}\\\\label{bincorol}\\nIf $X\\\\sim \\\\sum_{i=1}^k \\\\textup{Bin}(n,p_i)/k$ then $\\\\Pr[|S_{\\\\ell,t}-\\\\avg X^\\\\ell |\\\\geq \\\\gamma]= \\\\gamma^{-2}n^{2\\\\ell}/t$. \\n\\\\end{corollary}\\n\\nFixing $n$, the $\\\\ell^{\\\\textrm{th}}$ moment of a mixture of binomial distributions $X\\\\sim \\\\sum_{i=1}^k \\\\textup{Bin}(n,p_i)/k$ % the $\\\\ell^{th}$ moment of the random variable $X$ \\nis\\n\\\\[\\\\avg X^\\\\ell=\\\\sum_{i=1}^k f(p_i)/k\\\\] where $f$ is a polynomial of degree at most $\\\\ell$ with integer coefficients~\\\\citep{belkin2010polynomial}. If $p_i$ is an integer multiple of $\\\\epsilon$ then this implies $k(\\\\avg X^\\\\ell)/\\\\epsilon^\\\\ell$ is integral and therefore any mixture with a different $\\\\ell$th moment differs by at least $\\\\epsilon^\\\\ell/k$. Hence, learning the $\\\\ell$th moment up to $\\\\gamma_\\\\ell<\\\\epsilon^\\\\ell/(2k)$ implies learning the moment exactly. \\n\\\\begin{lemma}{\\\\label{lem:ind}}\\nFor $X\\\\sim \\\\textup{Bin}(n,p)$, $\\\\avg X^{\\\\ell}$ is a polynomial in $p$ of degree exactly $\\\\ell$ if $n \\\\ge \\\\ell$.\\n\\\\end{lemma}\\nThe proof of the lemma is relegated to the appendix.\\n\\\\begin{theorem}\\n$O(k^2  (n/\\\\epsilon)^{8/\\\\sqrt{\\\\epsilon}})$ samples are sufficient to exactly learn the first $4/\\\\sqrt{\\\\epsilon}$ moments of a uniform mixture of $k$ binomial distributions $\\\\sum_{i=1}^k \\\\textup{Bin}(n,p_i)/k$ with probability at least $7/8$ where each $p_i\\\\in \\\\{0,\\\\epsilon, 2\\\\epsilon, \\\\ldots, 1\\\\}$.\\n\\\\end{theorem}\\n\\\\begin{proof}\\nLet $T=4/\\\\sqrt{\\\\epsilon}$. From Corollary \\\\ref{bincorol} and the preceding discussion, learning the $\\\\ell $th moment exactly with failure probability $1/9^{1+T-\\\\ell}$ requires \\n\\\\[\\nt= \\\\gamma_\\\\ell^{-2} n^{2\\\\ell} 9^{1+T-\\\\ell} = O(k^2 9^{1+T-\\\\ell} n^{2\\\\ell}/\\\\epsilon^{2\\\\ell})= O(k^2 9^{T} (n/3\\\\epsilon)^{2\\\\ell})\\n\\\\]\\nsamples. And hence, we can compute all $\\\\ell$th moments exactly for $1\\\\leq \\\\ell\\\\leq 4/\\\\sqrt{\\\\epsilon}$ using \\\\[\\\\sum_{\\\\ell=1}^T O(k^2 9^{T} (n/3\\\\epsilon)^{2\\\\ell})=O(k^2 (n/\\\\epsilon)^{2T})\\\\] samples with failure probability $\\\\sum_{\\\\ell=1}^T 1/9^{1+T-\\\\ell}<\\\\sum_{i=1}^\\\\infty 1/9^{i}=1/8$.\\n\\\\end{proof}\\n\\n\\\\paragraph{How many moments determine the parameters}\\nIt remains to show the first $4/\\\\sqrt{\\\\epsilon}$ moments suffice to determine the $p_i$ values \\nin the mixture $X\\\\sim \\\\sum_{i=1}^k \\\\textup{Bin}(n,p_i)/k$ provided $n \\\\ge \\\\frac{4}{\\\\epsilon}$. To do this suppose there exists another mixture $Y\\\\sim \\\\sum_{i=1}^k \\\\textup{Bin}(n,q_i)/k$ and we will argue that \\n\\\\[\\n\\\\avg X^\\\\ell = \\\\avg Y^\\\\ell \\\\mbox{ for } \\\\ell=0, 1, \\\\ldots, 4\\\\sqrt{1/\\\\epsilon} \\n\\\\]\\nimplies $\\\\{p_i\\\\}_{i\\\\in [k]}=\\\\{q_i\\\\}_{i\\\\in [k]}$. To argue this, define integers $\\\\alpha_i,\\\\beta_i \\\\in \\\\{0,1,\\\\ldots, 1/\\\\epsilon\\\\}$ such at that $p_i=\\\\alpha_i \\\\epsilon$ and $q_i=\\\\beta_i \\\\epsilon$. Let $\\\\mathcal A=\\\\{\\\\alpha_1, \\\\ldots, \\\\alpha_k\\\\}$ and $\\\\mathcal B=\\\\{\\\\beta_1, \\\\ldots, \\\\beta_k\\\\}$ . Then,\\n\\\\[\\n\\\\avg X = \\\\avg Y   \\\\Longrightarrow   \\\\sum_i \\\\alpha_i=\\\\sum_i \\\\beta_i \\\\Longrightarrow m_1(\\\\mathcal A) = m_1(\\\\mathcal B) \\\\]\\nand, after some algebraic manipulation, it can be shown that for all $\\\\ell\\\\in \\\\{2,3,\\\\ldots\\\\}$, \\n\\\\begin{eqnarray*}\\n\\\\left (\\\\forall \\\\ell'\\\\in \\\\{0,1,\\\\ldots, \\\\ell-1\\\\}~,~ \\\\sum_i \\\\alpha_i^{\\\\ell'} =\\\\sum_i \\\\beta_i^{\\\\ell'} \\\\right )~\\\\mbox{ and }~ \\\\avg X^\\\\ell = \\\\avg Y^\\\\ell \\\\\\\\\\n \\\\Longrightarrow  \\\\left ( \\\\sum_i \\\\alpha_i^\\\\ell=\\\\sum_i \\\\beta_i^\\\\ell \\\\right ) \\\\Longrightarrow m_\\\\ell (\\\\mathcal A) = m_\\\\ell(\\\\mathcal B) \\\\ .\\n\\\\end{eqnarray*}\\nHence, if the first $T$ moments match $m_\\\\ell(\\\\mathcal A)= m_\\\\ell(\\\\mathcal B)$ for all $\\\\ell=0,1,\\\\ldots, T$. But the following theorem establishes that if $T=4\\\\sqrt{1/\\\\epsilon}$ then this implies $\\\\mathcal A=\\\\mathcal B$.\\n\\n\\n\\\\begin{theorem}[\\\\cite{KRASIKOV1997344}]\\\\label{kras}\\nFor any two subsets $S,T$ of $\\\\{0,1,\\\\dots,n-1\\\\}$, then \\n\\\\[S=T \\\\mbox{ iff } \\\\left (m_k(S) =  m_k(T)   \\\\mbox{ for all }  k=0,1,\\\\dots,4 \\\\sqrt{n} \\\\right )  \\\\ . \\\\]\\n\\\\end{theorem}\\n\\nWe note that the above theorem is essentially tight. Specifically, there exists $S\\\\neq T$ with $m_k(S) =  m_k(T) $ for $k=0,1,\\\\ldots, cn/\\\\log n$ for some $c$. As a consequence of this, we note that even the exact values of the $c\\\\sqrt{n}/\\\\log n$ moments are insufficient to learn the parameters of the distribution. For an example in terms of Gaussian mixtures, even given the promise $\\\\mu_i \\\\in \\\\{0,1, \\\\ldots, n-1\\\\}$ are distinct, then the first  $c\\\\sqrt{n}/\\\\log n$ moments of $\\\\sum_i \\\\mathcal{N}(\\\\mu_i,1)$ are insufficient to uniquely determine $\\\\mu_i$ whereas the first $4\\\\sqrt{n}$ moments are sufficient.\\n\\n\\n\\\\subsection{Extension to Non-Uniform Distributions}\\nWe now consider extending the framework to non-uniform distributions. In this case, the method of computing the moments is identical to the uniform case. However, when arguing that a small number of moments suffices we can no longer appeal to the Theorem \\\\ref{kras}.\\n\\nTo handle non-uniform distribution we introduce a precision variable $q$ and assume that the weights of the component distributions $\\\\omega_1, \\\\omega_2, \\\\ldots, \\\\omega_k$ are of the form:\\n\\\\[\\\\omega_i=\\\\frac{w_i}{\\\\sum_{i=1}^k w_i}\\\\]\\nwhere $w_i\\\\in \\\\{0,1,\\\\ldots, q-1\\\\}$. Then, in the above framework if we are trying to learn parameters $\\\\alpha_1, \\\\ldots, \\\\alpha_k$ then the moments are going to define a multi-set consisting of $w_i$ copies of $\\\\alpha_i$ for each $i\\\\in [k]$. To quantify how many moments suffice in this case, we need to prove a variant of Theorem \\\\ref{kras}. The proof is a relatively straight-forward generalization of proof by \\\\cite{Scott97} and can be found in the appendix.\\n\\n\\\\begin{theorem}\\\\label{thm:multiplicity}\\nFor any two multi-sets $S,T$ where each element is in  $\\\\{0,1,\\\\dots,n-1\\\\}$ and the multiplicity of each element is at most $q-1$, then $S=T$ if and only if  $m_k(S) =  m_k(T)   \\\\mbox{ for all }  k=0,1,\\\\dots,2 \\\\sqrt{qn\\\\log qn}$.\\n\\\\end{theorem}\\n\\n\\n\\n\\n\",\n",
              " '\\\\input{05-appendix}': \"\\\\section{Omitted Proofs}\\n\\n\\\\paragraph{Additional calculations for Lemma \\\\ref{lem:list}.}\\nWe consider each distribution in turn:\\n\\\\begin{itemize}\\n\\\\item \\\\emph{Gaussian:} Observe that $\\\\EE[G_t(X)]$ is precisely the characteristic function. Clearly we have $\\\\|G_t\\\\|_{\\\\infty} = 1$ and further\\n\\\\[\\\\EE[G_t(X)]=\\\\exp(it \\\\mu -\\\\sigma^2 t^2/2)=\\\\exp(-\\\\sigma^2 t^2/2) z^\\\\mu.\\\\]\\n\\n\\\\item \\\\emph{Poisson:} If $G_t(x) = (1+it)^x$ then since $|1+it|^2 = 1 + t^2$ the second claim follows. For the first:\\n\\\\begin{align*}\\n\\\\EE[G_t(X)]=\\\\exp(\\\\lambda((1+it)-1))=z^\\\\lambda.\\\\tag*\\\\qedhere\\n\\\\end{align*}\\n\\n\\\\item \\\\emph{Chi-Squared:} Let $w_t=\\\\exp(1/2-e^{-2it}/2)$ then $|w_t|^{2}=|e^{1-e^{-2it}}|=|e^{1-\\\\cos 2t}e^{i\\\\sin 2t}|  \\\\le e^{ct^{2}+O(t^{4})}$ and\\n \\\\[\\\\EE[G_t(X)]=(1-2\\\\ln w_t)^{-\\\\frac{\\\\ell}{2}}=z^\\\\ell.\\\\]\\n\\\\item \\\\emph{Negative Binomial:} Let $w_t=1/p-(1/p-1)e^{-it}$ then $ |w_t|^{2}=\\\\frac{1+(1-p)^{2}-2(1-p)\\\\cos t}{p^{2}}  = \\n\\\\frac{p^2+4(1-p)\\\\sin^2(t/2)}{p^2}\\\\le e^{\\\\frac{(1-p)t^2}{p^2}}$ and\\n\\\\[\\\\EE[G_t(X)]= \\\\Big(\\\\frac{1-p}{1-pw_t }\\\\Big)^{r}=z^r.\\\\]\\n\\\\end{itemize}\\n\\n\\n\\\\subsection{Additional calculations for Theorem~\\\\ref{thm:tv}.}\\n\\n\\\\begin{itemize} \\n\\\\item \\\\emph{Gaussian:}\\nThe characteristic function of a Gaussian $X \\\\sim \\\\mathcal{N}(\\\\mu,\\\\sigma^2)$ is \\n\\\\begin{align*}\\nC_\\\\Nc(t)=\\\\EE e^{it X}=e^{it\\\\mu-\\\\frac{t^{2}\\\\sigma^{2}}{2}}.\\n\\\\end{align*}   \\nTherefore we have that \\n\\\\begin{align*}\\nC_\\\\Mc(t)-C_{\\\\Mc'}(t) \\\\ge \\\\frac{e^{-\\\\frac{t^{2}\\\\sigma^{2}}{2}}}{k}    \\\\sum_{j =1}^k (e^{it \\\\mu_j}-  e^{it \\\\mu_j'}) .\\n\\\\end{align*}\\nNow, using Lemma~\\\\ref{lem:npb}, there exist an absolute constant $c$ such that,  %$\\\\Big(\\\\sum_{a \\\\in A} e^{ita}- \\\\sum_{b \\\\in B} e^{itb} \\\\Big)$ to be a Littlewood polynomial such that \\n\\\\begin{align*}\\n\\\\max_{-\\\\frac{\\\\pi}{\\\\epsilon L}\\\\le t \\\\le \\\\frac{\\\\pi}{\\\\epsilon L}} \\\\big| \\\\sum_{j =1}^k (e^{it \\\\mu_j}-  e^{it \\\\mu_j'})\\\\big| \\\\ge e^{-cL}.\\n\\\\end{align*}\\nAlso, for $t \\\\in (-\\\\frac{\\\\pi}{\\\\epsilon L}, \\\\frac{\\\\pi}{\\\\epsilon L}),$ $e^{-\\\\frac{t^{2} \\\\sigma^{2}}{2}} \\\\ge e^{-\\\\frac{\\\\sigma^2\\\\pi^2}{2\\\\epsilon^2 L^2}}.$ And therefore,\\n\\\\begin{align*}\\n\\\\Big|C_\\\\Mc(t)-C_{\\\\Mc'}(t)\\\\Big| \\\\ge \\\\frac{1}{k}  e^{-\\\\frac{\\\\sigma^2\\\\pi^2}{2\\\\epsilon^2 L^2}-cL}.\\n\\\\end{align*}\\nBy substituting $L = \\\\frac{(\\\\pi\\\\sigma)^{2/3}}{(\\\\epsilon^2c)^{1/3}}$ above we conclude that there exists $t$ such that \\n\\\\begin{align*}\\n\\\\Big|C_\\\\Mc(t)-C_{\\\\Mc'}(t)\\\\Big| \\\\ge \\\\frac{1}{k}  e^{-\\\\frac32(c\\\\pi\\\\sigma/\\\\epsilon)^{2/3}}.\\n\\\\end{align*}\\nNow using \\n Lemma~\\\\ref{lem:chartv}, we have\\n$\\\\variation{\\\\mathcal{M}' -\\\\mathcal{M}} \\\\geq k^{-1} \\\\exp(-\\\\Omega((\\\\sigma/\\\\epsilon)^{2/3}))$.\\n\\n\\\\item \\\\emph{Chi-Squared:}\\nLet $X \\\\sim \\\\Mc$ and $X' \\\\sim \\\\Mc'$. Then, for $w=\\\\exp(1/2-e^{-2it}/2)$, from Lemma~\\\\ref{lem:list},\\n$$\\n\\\\EE(w^X) - \\\\EE(w^{X'}) = \\\\frac1k\\\\sum_{j=1}^k (e^{it \\\\ell_j} - e^{it \\\\ell'_j}) .\\n$$\\nNow we use  \\n Lemma~\\\\ref{lem:chartv}, with $\\\\Omega' = [0, 2N]$ we have,\\n  \\\\begin{align*}\\n\\\\|\\\\Mc -\\\\Mc'\\\\|_{TV} \\\\ge   e^{-2ct^2 N} \\\\Big(\\\\left| \\\\EE(w^X) - \\\\EE(w^{X'}) \\\\right|- \\\\int_{x >2N}\\\\exp(ct^2x) f(x) dx\\\\Big),\\n  \\\\end{align*}\\n  where $f\\\\sim \\\\chi^2(N)$.\\n We have,\\n\\\\begin{align*}\\n \\\\int_{x >2N}\\\\exp(ct^2x) f(x) dx& = \\\\frac{1}{(1-2ct^2)^{N/2-1}}\\\\int_{y >2N(1-2ct^2)}f(y)dy \\\\le \\\\frac{e^{-N(1-4ct^2)^2/8}}{(1-2ct^2)^{N/2-1}}\\\\\\\\\\n & \\\\le \\\\exp(-\\\\Omega(N)),\\n\\\\end{align*}\\nwhere we have used the pdf of chi-squared distribution and the tail bounds for chi-squared.\\nNow using Lemma~\\\\ref{lem:npb}, and taking $|t| \\\\le \\\\frac{\\\\pi}{L}$,\\n$$\\n \\\\|\\\\Mc -\\\\Mc'\\\\|_{TV} \\\\ge  k^{-1} e^{-c'L-2ct^2N} - \\\\exp(-\\\\Omega(n)) \\\\ge  k^{-1}\\\\exp( -c'L-2\\\\pi^2N/L^2)- \\\\exp(-\\\\Omega(N)).\\n$$\\nAgain setting, $L = N^{1/3}$,\\n$$\\n \\\\|\\\\Mc -\\\\Mc'\\\\|_{TV} \\\\ge  k^{-1} \\\\exp(- \\\\Omega(N^{1/3})).\\n$$\\n \\n\\\\item \\\\emph{Negative-Binomial:} \\nLet $X \\\\sim \\\\Mc$ and $X' \\\\sim \\\\Mc'$. Then, for $w=1/p - (1/p - 1)e^{-it}$, from Lemma~\\\\ref{lem:list}, taking $G(x) = w^x$,\\n$$\\n\\\\EE(w^X) - \\\\EE(w^{X'}) = \\\\frac1k\\\\sum_{j=1}^k (e^{it r_j} - e^{it r'_j}) .\\n$$\\nNow we use  \\n Lemma~\\\\ref{lem:chartv}, with $\\\\Omega' = [0, 6pN/(1-p)]$ we have,\\n  \\\\begin{align*}\\n\\\\|\\\\Mc -\\\\Mc'\\\\|_{TV} \\\\ge   e^{-12ct^2N/p} \\\\Big(\\\\left| \\\\EE(w^X) - \\\\EE(w^{X'}) \\\\right|- \\\\sum_{x >\\\\frac{6Np}{1-p}}|w|^xu(x)\\\\Big),\\n  \\\\end{align*}\\n  where $u(x) =  \\\\binom{x+N-1}{x}(1-p)^Np^x$. We have  $|w| \\\\le e^{c(1-p)t^2/p^2} \\\\le e^{c(1-p)/p^2}$ for $t<1$.\\n Using Lemma~\\\\ref{lem:tail}, with $X\\\\sim NB(N,p)$, we have,\\n\\\\begin{align*}\\n\\\\sum_{x >\\\\frac{6Np}{1-p}}\\\\exp(cx(1-p)/p^2)u(x)& \\\\le a^{1-\\\\frac{6Np}{1-p}}\\\\EE[a^{2X}] = a^{1-\\\\frac{6Np}{1-p}}\\\\Big(\\\\frac{1-p}{1-pa^2}\\\\Big)^N = \\\\exp(-\\\\Omega(N)),\\n\\\\end{align*}\\nwhere, $ a = \\\\exp(c(1-p)/p^2)>1$.\\nNow using Lemma~\\\\ref{lem:npb}, and taking $|t| \\\\le \\\\frac{\\\\pi}{L}$,\\n\\\\begin{align*}\\n \\\\|\\\\Mc -\\\\Mc'\\\\|_{TV}& \\\\ge  k^{-1} e^{-c'L-12ct^2N/p} - \\\\exp(-\\\\Omega(n)) \\\\\\\\\\n &\\\\ge  k^{-1}\\\\exp( -c'L-12\\\\pi^2N/(pL^2))- \\\\exp(-\\\\Omega(N)).\\n\\\\end{align*}\\n Setting $L = (N/p)^{1/3}$,\\n$$\\n \\\\|\\\\Mc -\\\\Mc'\\\\|_{TV} \\\\ge  k^{-1} \\\\exp(- \\\\Omega((N/p)^{1/3})).\\n$$\\n\\\\end{itemize}\\n\\n\\\\subsection{Proof of Theorem \\\\ref{thm:multiplicity}}\\n\\n\\\\input{proofmultiplicity}\\n\\n\\n\\\\subsection{Algebraic method for Geometric distribution}\\n\\\\input{geometric}\\n\\\\subsection{Proof of Lemma \\\\ref{lem:ind}}\\nWe will prove that for $X\\\\sim \\\\textup{Bin}(n,p)$, the leading term of $\\\\avg X^{\\\\ell}$ is  $\\\\prod_{i=0}^{\\\\ell-1} (n-i)p^{\\\\ell}$. Since for $n \\\\geq \\\\ell$, $\\\\prod_{i=0}^{\\\\ell-1} (n-i) \\\\neq 0$, this implies that $\\\\avg X^{\\\\ell}$ is a polynomial of degree exactly $\\\\ell$.\\nWe will prove this by induction. Since $X\\\\sim \\\\textup{Bin}(n,p)$, we know that  \\n$\\n\\\\avg X= np.\\n$\\nThis verifies the base case. Now, in the induction step, let us assume that the leading term of $\\\\avg X^{k}$ is $\\\\prod_{i=0}^{k-1} (n-i)p^{k}$. It is known that (see \\\\cite{belkin2010polynomial}) \\n\\\\begin{align*}\\n\\\\avg X^{k+1} =np \\\\avg X^k+p(1-p)\\\\frac{d \\\\avg X^{k}}{dp}.  \\n\\\\end{align*}\\nTherefore it follows that the leading term of $\\\\avg X^{k+1}$ is  \\n\\\\begin{align*}\\nnp\\\\prod_{i=0}^{k-1} (n-i)p^{k}-kp^2\\\\prod_{i=0}^{k-1} (n-i)p^{k-1}=\\\\prod_{i=0}^{k} (n-i)p^{k+1}.\\n\\\\end{align*}\\nThis proves the induction step and the lemma.\\n\\n\\\\remove{\\n\\\\section{Extension to higher dimensions}\\n\\\\subsection{Technique 1: Projection on one dimension}\\nIn this section, we will show how to extend our results for univariate mixtures to higher dimensions. As an example, we will prove our results for mixtures of multi-variate Gaussians with a shared co-variance matrix but we want to stress over here that our techniques are general and can be used to extend all the analytic results that we have shown earlier in the paper. We will use $\\\\mathcal{N}(\\\\f{\\\\mu},\\\\f{\\\\Sigma})$ to denote a multivariate Gaussian distribution defined on $\\\\mathbb{R}^d$ where $\\\\f{\\\\mu} \\\\in \\\\mathbb{R}^d$ is the mean and $\\\\f{\\\\Sigma}\\\\in \\\\mathbb{R}^{d \\\\times d}$ is the covariance matrix. We also assume that the means $\\\\{\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$ have precision $\\\\epsilon$ i.e. every entry of every mean belongs to $\\\\epsilon \\\\mathbb{Z}$. We now have the following theorem:\\n\\\\begin{theorem}[Learning Multi-variate Gaussian mixtures]{\\\\label{thm:main2}}\\nLet $\\\\ca{M}=\\\\frac{1}{k}\\\\sum_{i=1}^k\\\\mathcal{N}(\\\\f{\\\\mu}_i,\\\\f{\\\\Sigma})$\\nbe a uniform mixture of $k$ $d$-dimensional Gaussians with known shared\\ncovariance matrix $\\\\f{\\\\Sigma}$ having maximum eigenvalue $\\\\lambda^{\\\\star}$ and with distinct means $\\\\f{\\\\mu}_i \\\\in\\n(\\\\epsilon\\\\mathbb{Z})^d$. Then there exists an algorithm that requires $O\\\\Big(\\\\exp((\\\\lambda^{\\\\star}d^2k^8/\\\\epsilon)^{2/3})\\\\Big)$ samples from $\\\\ca{M}$ and\\nexactly identifies the means $\\\\{\\\\f{\\\\mu}_i\\\\}_{i=1}^k$ with high\\nprobability.\\n\\\\end{theorem}\\n\\n\\\\paragraph{Overview:} We will project the samples from $\\\\ca{M}$ randomly onto a direction (vector $\\\\in \\\\mathbb{R}^d$) $\\\\f{r}$ whose entries are sampled uniformly and independently from the set\\n\\\\begin{align*}\\n\\\\ca{T} \\\\equiv \\\\{0,1,\\\\dots,dk^4-1,dk^4\\\\}.\\n\\\\end{align*} \\nIn that case the projected samples are equivalent to obtaining samples from the univariate Gaussian mixture $\\\\ca{M}^{\\\\f{r}}$ where\\n $$\\\\mathcal{M}^{\\\\f{r}}=\\\\frac{1}{k}\\\\sum_{i=1}^k\\\\ca{N}(\\\\f{r}^{T}\\\\boldsymbol{\\\\mu}_i,\\\\f{r}^{T}\\\\mathbf{\\\\Sigma}\\\\f{r}).$$\\n Subsequently, we can recover the parameters $\\\\{\\\\f{r}^{T}\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$ by invoking Theorem \\\\ref{thm:main} and thus we can obtain a linear constraint on the parameters. However, there is an additional issue of alignment because for two distinct directions $\\\\f{r}_1$ and $\\\\f{r}_2$, we need to figure out the recovered parameters that correspond to the same component in the Gaussian mixture. In order to align, we will also project on the direction $\\\\f{r}_1+\\\\f{r}_2$ and therefore recover the parameters $\\\\{(\\\\f{r}_1+\\\\f{r}_2)^{T}\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$. Subsequently, we will claim that an element in $\\\\{\\\\f{r}_1^{T}\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$ and an element in $\\\\{\\\\f{r}_2^{T}\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$ corresponds to the same component in the mixture if their sum belongs to the set $\\\\{(\\\\f{r}_1+\\\\f{r}_2)^{T}\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$. Therefore, we will first find a direction $\\\\f{r}_1$ such that the recovered parameters $\\\\{\\\\f{r}_1^{T}\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$ are distinct and subsequently we will consider $\\\\f{r}_1$ to be fixed. Next we will define a direction $\\\\f{r}$ to be \\\\textit{good} with respect to $\\\\f{r}_1$ if we can correctly align $\\\\f{r}$ and $\\\\f{r}_1$ i.e. correctly identify the projected means which correspond to the same component of the mixture $\\\\ca{M}$ for all the $k$ components. We will show that with high probability, the next $d-1$ directions $\\\\f{r}_2,\\\\dots,\\\\f{r}_d$ uniformly and independently sampled from $\\\\ca{T}^{d}$ are \\\\textit{good} directions with respect to $\\\\f{r}_1$. Since we have correctly aligned $\\\\f{r}_2,\\\\dots,\\\\f{r}_d$ with $\\\\f{r}_1$, we now have $d$ linear constraints on each of $\\\\{\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$ and if all of $\\\\f{r}_1,\\\\f{r}_2,\\\\dots,\\\\f{r}_d$ are linearly independent, then we can recover the parameters $\\\\{\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$. \\\\\\\\\\\\\\\\\\n \\n\\\\paragraph{Proof of Theorem \\\\ref{thm:main2}}  \\n\\nWe will randomly choose $d$ directions $\\\\f{r}_1,\\\\f{r}_2,\\\\dots,\\\\f{r}_d$ such that its entries are uniformly and independently sampled from $\\\\ca{T}$. \\nTherefore, for a particular direction $\\\\f{r}$, we must have that \\n\\\\begin{align*}\\n\\\\Pr(\\\\f{r}^{T}\\\\boldsymbol{\\\\mu}_i=\\\\f{r}^{T}\\\\boldsymbol{\\\\mu}_j) \\\\le \\\\frac{1}{dk^4+1}.\\n\\\\end{align*}\\nWe can take a union bound over all distinct pairs of $i,j \\\\in [k]$ to show that\\n\\\\begin{align*}\\n\\\\Pr(\\\\{\\\\f{r}^{T}\\\\boldsymbol{\\\\mu}_i\\\\}_{i=1}^{k} \\\\text{ are distinct }) > 1- \\\\frac{k^{2}}{dk^4+1}.\\n\\\\end{align*}\\nFurther taking a union bound over all $d$ directions, we can conclude that the means $\\\\{\\\\f{\\\\mu}_i\\\\}_{i=1}^{k}$ projected on $\\\\f{r}_j^{T}$ are distinct for all $j=1,2,\\\\dots,d$ with probability at least $1-(1/k^2)$. Next, we want to evaluate the probability that $\\\\f{r}_2$ is \\\\textit{good} with respect to $\\\\f{r}_1$ where the randomness is over $\\\\f{r}_2$ and $\\\\f{r}_1$ is considered to be fixed. For a triplet of indices $(i_1,i_2,i_3) \\\\in [k]$ such that not all of them are identical, we must have \\n\\\\begin{align*}\\n\\\\Pr(\\\\f{r}_1^{T}\\\\mu_{i_1}+\\\\f{r}_2^{T}\\\\mu_{i_2}=(\\\\f{r}_1+\\\\f{r}_2)^{T}\\\\mu_{i_3})\\n\\\\end{align*}\\nto be atmost $1/(dk^{3}+1)$. Taking a union bound over all triplets $(i_2,i_2,i_3)$ which are not identical, the probability that $\\\\f{r}_2$ is not \\\\textit{good} with respect to $\\\\f{r}_1$ is atmost $1/dk$. Subsequently, taking a union bound, we get that all the $d-1$ directions $\\\\f{r}_2,\\\\f{r}_3,\\\\dots,\\\\f{r}_d$ are \\\\textit{good} with respect to $\\\\f{r}_1$ with probability at least $1-(1/k)$. Notice that the fact $\\\\f{r}_2,\\\\dots,\\\\f{r}_d$ being \\\\textit{good} with respect to $\\\\f{r}_1$ also implies that the means $\\\\f{\\\\mu}_1,\\\\dots,\\\\f{\\\\mu}_d$ projected on $\\\\f{r}_1+\\\\f{r}_j$ are distinct for all $j=1,2,\\\\dots,d$. \\nTherefore, at this point we can invoke Theorem \\\\ref{thm:main} to conclude that we can recover $\\\\{\\\\f{r}^{T}\\\\f{\\\\mu}_j\\\\}_{j=1}^{k}$ for all the direction vectors $$\\\\f{r} \\\\in \\\\{\\\\f{r}_1,\\\\f{r}_2,\\\\dots,\\\\f{r}_d\\\\} \\\\cup \\\\{\\\\f{r}_1+\\\\f{r}_2,\\\\f{r}_1+\\\\f{r}_3,\\\\dots,\\\\f{r}_1+\\\\f{r}_d\\\\}$$ with high probability. Since the entries in all the direction vectors are integral, hence the projected means also have a precision of $\\\\epsilon$ and moreover, the variance of the projected mixture is upper bounded as \\n\\\\begin{align*}\\n\\\\max_{\\\\f{r}} \\\\f{r}^{T} \\\\f{\\\\Sigma} \\\\f{r} \\\\le \\\\lambda^{\\\\star} ||\\\\f{r}||_{2}^{2} \\\\le \\\\lambda^{\\\\star}d^2k^8. \\n\\\\end{align*}  \\nTherefore, invoking Theorem \\\\ref{thm:main} and plugging in the upper bound on the variance and the precision, we would need $O\\\\Big(\\\\exp((\\\\lambda^{\\\\star}d^2k^8/\\\\epsilon)^{2/3})\\\\Big)$ samples to recover all the projected means correctly with high probability.\\nMoreover, we can consider $\\\\{\\\\f{r}_1^{T}\\\\f{\\\\mu}_j\\\\}_{j=1}^{k}$ to be $k$ labels and since $\\\\f{r}_2,\\\\dots,\\\\f{r}_d$ is \\\\textit{good} with respect to $\\\\f{r}_1$, we can assign a label correctly to each mean projected on one of $\\\\f{r}_2,\\\\dots,\\\\f{r}_d$ and therefore recover $\\\\{\\\\f{r}_{i}^{T}\\\\f{\\\\mu}_j\\\\}_{i=1}^{d}$ for all $j=1,2,\\\\dots,k$. Finally, since \\n\\\\begin{align*}\\n\\\\Pr(\\\\f{r}_1,\\\\dots,\\\\f{r}_d \\\\text{ linearly independent over $\\\\mathbb{R}$}) \\\\ge \\\\Pr(\\\\f{r}_1,\\\\dots,\\\\f{r}_d \\\\text{ linearly independent over $\\\\mathbb{F}_{dk^4+1}$}),\\n\\\\end{align*}\\nthe probability of $\\\\f{r}_1,\\\\f{r}_2,\\\\dots,\\\\f{r}_d$ being linearly independent is at least \\n\\\\begin{align*}\\n\\\\prod_{i=1}^{d}\\\\Big(1-\\\\frac{1}{(dk^4+1)^{i}}\\\\Big).\\n\\\\end{align*}\\nIf all $\\\\f{r}_1,\\\\dots,\\\\f{r}_d$ are linearly independent, then we can solve and therefore recover all the means $\\\\f{\\\\mu}_1,\\\\dots,\\\\f{\\\\mu}_k$.\\n\\n\\\\subsection{Technique 2: Complex analysis in high dimension}\\nIn this section, we will show how to extend our results for univariate mixtures to higher dimensions by directly applying complex analysis machinery in the higher dimensions. As an example, we will again prove our results for mixtures of multi-variate Gaussians $\\\\ca{M}=\\\\frac{1}{k}\\\\sum_{i=1}^k\\\\mathcal{N}(\\\\f{\\\\mu}_i,\\\\f{\\\\Sigma})$ with means $\\\\f{\\\\mu}_1,\\\\f{\\\\mu}_2,\\\\dots,\\\\f{\\\\mu}_k \\\\in \\\\mathbb{R}^d$ and a shared co-variance matrix $\\\\f{\\\\Sigma}\\\\in \\\\mathbb{R}^{d\\\\times d}$. We have the following theorem:\\n\\\\begin{theorem}[Learning Multi-variate Gaussian mixtures]{\\\\label{thm:main3}}\\nLet $\\\\ca{M}=\\\\frac{1}{k}\\\\sum_{i=1}^k\\\\mathcal{N}(\\\\f{\\\\mu}_i,\\\\f{\\\\Sigma})$\\nbe a uniform mixture of $k$ $d$-dimensional Gaussians with known shared\\ncovariance matrix $\\\\f{\\\\Sigma}$ having maximum eigenvalue $\\\\lambda^{\\\\star}$ and with distinct means $\\\\f{\\\\mu}_i \\\\in\\n(\\\\epsilon\\\\mathbb{Z})^d$. Then there exists an algorithm that requires $$k^3\\\\exp\\\\Bigg(O\\\\Big(\\\\Big(\\\\frac{\\\\lambda^{\\\\star}d(\\\\log k)^{1/d}}{\\\\epsilon^{2}}\\\\Big)^{\\\\frac{d}{d+2}}\\\\Big)\\\\Bigg) $$ samples from $\\\\ca{M}$ and\\nexactly identifies the means $\\\\{\\\\f{\\\\mu}_i\\\\}_{i=1}^k$ with high\\nprobability.\\n\\\\end{theorem}\\nWe will begin by showing an extension of Lemma \\\\ref{lem:npb} for higher dimensions that was also proved in \\\\cite{krishnamurthy2019trace} for $d=2$ but we provide the details here for the sake of completeness. \\n\\\\begin{lemma}{\\\\label{lem:npbext}}\\nLet $f(z_1,z_2,\\\\dots,z_d)$ be a non-zero polynomial whose coefficients are in $\\\\{-1,0,+1\\\\}$ and has atmost $2k$ non-zero terms. In that case, \\n\\\\begin{align*}\\n\\\\left|f(z_1^{\\\\star},z_2^{\\\\star},\\\\dots,z_d^{\\\\star})\\\\right| \\\\ge \\\\exp(-C_1L^d \\\\log k)\\n\\\\end{align*}\\nfor some $z_1^{\\\\star}=\\\\exp(i\\\\theta_1),z_2^{\\\\star}=\\\\exp(i\\\\theta_2),\\\\dots,z_d^{\\\\star}=\\\\exp(i\\\\theta_d)$ where $|\\\\theta_1|,|\\\\theta_2|,\\\\dots,|\\\\theta_d| \\\\le \\\\pi/L$ and $C_1$ is a universal constant.\\n\\\\end{lemma}\\n\\\\begin{proof}\\nFix $L > 0$ and define the polynomial\\n\\\\begin{align*}\\nF(z_1,z_2,\\\\dots,z_d)=\\\\prod_{1\\\\leq a_1,a_2,\\\\dots,a_d \\\\leq L} f( z_1 e^{\\\\pi i a_1 /L}, z_2 e^{\\\\pi i a_2 /L},\\\\dots,z_d e^{\\\\pi i a_d /L} ).\\n\\\\end{align*}\\nWe first show that there exists $z^\\\\star_1,z^\\\\star_2,\\\\dots,z^\\\\star_d$ on the unit disk ($|z^{\\\\star}_1|=|z^{\\\\star}_2|=\\\\dots=|z^{\\\\star}_d|=1$) such that\\n$F(z^\\\\star_1,z^\\\\star_2,\\\\dots,z^\\\\star_d) \\\\geq 1$. This follows from an iterated application of the\\nmaximum modulus principle. First factorize $F(z_1,z_2,\\\\dots,z_d) = z_d^{s_d}\\nF^1(z_1,z_2,\\\\dots,z_d)$ where $s_d$ is chosen such that $F^1(z_1,z_2,\\\\dots,z_d)$ has no common\\nfactors of $z_d$. Since $F$ has non-zero coefficients, this implies\\nthat $F^1(z_1,z_2,\\\\dots,0)$ is a non-zero polynomial and therefore using the maximum modulus principle, there exists a value of $z_d=z_d^{\\\\star}$ such that $|z_d^{\\\\star}|=1$ and therefore \\n\\\\begin{align*}\\n|z^{\\\\star}_d|^{s_d}|F^1(z_1,z_2,\\\\dots,z^{\\\\star}_d)| \\\\ge |F^1(z_1,z_2,\\\\dots,0)|. \\n\\\\end{align*}\\nSubsequently we can further factorize $F^1(z_1,z_2,\\\\dots,0)=z_{d-1}^{s_{d-1}}F^2(z_1,z_2,\\\\dots,z_{d-1})$ so that $F_2(z_1,z_2,\\\\dots,z_{d-1})$ has no common factors in $z_{d-1}$. Repeating this procedure $d$ times, we can show the following chain of inequalities \\n\\\\begin{align*}\\n&|F(z^\\\\star_1,z^\\\\star_2,\\\\dots,z^{\\\\star}_d)| = |F^1(z^\\\\star_1,z^\\\\star_2,\\\\dots,z^{\\\\star}_d)| \\\\geq |F^1(z^\\\\star_1,\\\\dots,0)| \\\\\\\\\\n &= |F^2(z^\\\\star_1,z^\\\\star_2,\\\\dots,z^{\\\\star}_{d-1})| \\\\geq |F^2(z^\\\\star_1,\\\\dots,0)| \\\\geq \\\\dots |F^d(z^{\\\\star}_1)|\\\\ge |F^d(0)| \\\\ge 1 \\n\\\\end{align*}\\n\\n\\\\begin{lemma}\\nConsider two distinct uniform mixture of $d$-dimensional gaussians $\\\\ca{M}$ and $\\\\ca{M}'$ having $k$ components each and consider the set $\\\\ca{A}_{\\\\ca{M},\\\\ca{M}'}$ where \\n\\\\begin{align*}\\n\\\\ca{A}_{\\\\ca{M},\\\\ca{M}'} \\\\equiv \\\\{x \\\\in \\\\mathbb{R}^d \\\\mid \\\\ca{M}(x) \\\\ge \\\\ca{M}'(x)\\\\}.\\n\\\\end{align*}\\nThe VC dimension of the class $\\\\ca{A} \\\\equiv \\\\{\\\\ca{A}_{\\\\ca{M},\\\\ca{M}'} \\\\text{ for any two distinct mixtures } \\\\ca{M},\\\\ca{M}'\\\\}$ is $O(k)$.\\n\\\\end{lemma}\\n\\\\begin{proof}\\n\\\\end{proof}\\n\\n\\nNow, for any $a_1,a_2,\\\\dots,a_d \\\\in \\\\{1,\\\\ldots,L\\\\}$ we have\\n\\\\begin{align*}\\n1 \\\\leq |F(z_1^\\\\star,z_2^\\\\star,\\\\dots,z_d^{\\\\star})| \\\\leq |f(z_1^\\\\star e^{\\\\pi ia_1/L}, z_2^\\\\star e^{\\\\pi i a_2/L},\\\\dots, z_d^\\\\star e^{\\\\pi i a_d/L})| \\\\cdot (2k)^{(L^d-1)},\\n\\\\end{align*}\\nwhere we are using the fact that $|f(z_1,z_2,\\\\dots,z_d)| \\\\leq 2k$. This proves the lemma, since we may choose $a_1,a_2,\\\\dots,a_d$ such that $z_j^\\\\star e^{\\\\pi ia_j /L} = \\\\exp(i\\\\theta_j)$ for $|\\\\theta_j| \\\\leq \\\\pi/L$ for all $j=1,2,\\\\dots,d$. \\n\\\\end{proof}\\n\\n\\\\paragraph{Proof of Theorem \\\\ref{thm:main3}} \\nThe characteristic function of a Gaussian $X \\\\sim \\\\mathcal{N}(\\\\f{\\\\mu},\\\\f{\\\\Sigma})$ is \\n\\\\begin{align*}\\nC_\\\\mathcal{N}(\\\\f{t})=\\\\EE e^{i\\\\f{t}^{T} X}=e^{i\\\\f{t}^{T}\\\\mu-\\\\frac{\\\\f{t}^{T}\\\\f{\\\\Sigma}\\\\f{t}}{2}}.\\n\\\\end{align*}   \\nTherefore, we have that for two mixtures $\\\\ca{M},\\\\ca{M}'$ such that its mean parameters are distinct, we must have\\n\\\\begin{align*}\\nC_{\\\\mathcal{M}}(\\\\f{t})-C_{\\\\mathcal{M'}}(\\\\f{t}) \\\\ge \\\\frac{e^{-\\\\frac{\\\\f{t}^{T}\\\\f{\\\\Sigma}\\\\f{t}}{2}}}{k} \\\\sum_{i =1}^k (e^{i\\\\f{t}^{T}\\\\mu_i}-  e^{i\\\\f{t}^{T} \\\\mu_i'}) .\\n\\\\end{align*}\\nLet us denote by $\\\\f{t}_j$ to be the $j^{th}$ entry of $\\\\f{t}$, $\\\\mu_{i,j}$ denote the $j^{th}$ entry of the mean $\\\\mu_i$ and let $z_j=e^{i\\\\f{t}_j}$ be a complex number that belongs to circumference of the unit circle. In that case, we can rewrite\\n\\\\begin{align*}\\nC_{\\\\mathcal{M}}(\\\\f{t})-C_{\\\\mathcal{M'}}(\\\\f{t}) \\\\ge \\\\frac{e^{-\\\\frac{\\\\f{t}^{T}\\\\f{\\\\Sigma}\\\\f{t}}{2}}}{k} \\\\sum_{i =1}^k (\\\\prod_{j=1}^{d} z_j^{\\\\f{\\\\mu}_{i,j}}- \\\\prod_{j=1}^{d} z_j^{\\\\f{\\\\mu}_{i',j}}) .\\n\\\\end{align*}\\n\\nNow, using Lemma~\\\\ref{lem:npbext}, there exist an absolute constant $c$ such that,  %$\\\\Big(\\\\sum_{a \\\\in A} e^{ita}- \\\\sum_{b \\\\in B} e^{itb} \\\\Big)$ to be a Littlewood polynomial such that \\n\\\\begin{align*}\\n\\\\max_{ z_1,z_2,\\\\dots,z_d \\\\in \\\\{e^{i\\\\theta}:|\\\\theta| \\\\le \\\\frac{\\\\pi}{L}\\\\}} \\\\big| \\\\sum_{i =1}^k (\\\\prod_{j=1}^{d} z_j^{\\\\f{\\\\mu}_{i,j}}- \\\\prod_{j=1}^{d} z_j^{\\\\f{\\\\mu}_{i',j}}) \\\\big| \\\\ge e^{-cL^d \\\\log k}.\\n\\\\end{align*}\\nAlso, for $z_1,z_2,\\\\dots,z_d \\\\in \\\\{e^{i\\\\theta}:|\\\\theta| \\\\le \\\\frac{\\\\pi}{L}\\\\}$, we must have\\n\\\\begin{align*}\\ne^{-\\\\frac{\\\\f{t}^{T}\\\\f{\\\\Sigma}\\\\f{t}}{2}} \\\\ge e^{-\\\\frac{\\\\lambda^{\\\\star}||\\\\f{t}||_2^2}{2}} \\\\ge e^{-\\\\frac{\\\\lambda^{\\\\star}d\\\\pi^2}{2\\\\epsilon^2L^2}}\\n\\\\end{align*}\\nwhere $\\\\lambda^{\\\\star}$ is the largest eigenvalue of the matrix $\\\\f{\\\\Sigma}$. Therefore, \\n\\\\begin{align*}\\n\\\\Big|C_\\\\Mc(\\\\f{t})-C_{\\\\Mc'}(\\\\f{t})\\\\Big| \\\\ge \\\\frac{1}{k}  \\\\exp\\\\Big(-\\\\frac{\\\\lambda^{\\\\star}d\\\\pi^2}{2\\\\epsilon^2L^2}-cL^d\\\\log k\\\\Big).\\n\\\\end{align*}\\nBy substituting $L = \\\\Big(\\\\frac{(\\\\pi\\\\sigma)^{2}}{(\\\\epsilon^2c)}\\\\Big)^{1/(d+2)}$ above we conclude that there exists $\\\\f{t}$ such that \\n\\\\begin{align*}\\n\\\\Big|C_\\\\Mc(\\\\f{t})-C_{\\\\Mc'}(\\\\f{t})\\\\Big| \\\\ge \\\\frac{1}{k}  \\\\exp\\\\Big(-c'(\\\\log k)^{\\\\frac{2}{d+2}}\\\\Big(\\\\frac{\\\\lambda^{\\\\star}d}{\\\\epsilon^{2}}\\\\Big)^{\\\\frac{d}{d+2}}\\\\Big).\\n\\\\end{align*}\\nwhere $c'$ is some absolute constant $>0$.\\nNow using \\n Lemma~\\\\ref{lem:chartv}, we have\\n$$\\\\variation{\\\\mathcal{M}' -\\\\mathcal{M}} \\\\geq k^{-1} \\\\exp\\\\Bigg(-\\\\Omega\\\\Big(\\\\Big(\\\\frac{\\\\lambda^{\\\\star}d(\\\\log k)^{1/d}}{\\\\epsilon^{2}}\\\\Big)^{\\\\frac{d}{d+2}}\\\\Big)\\\\Bigg).$$\\nSuppose we want to learn the parameters of the mixture \\n\\\\[\\\\mathcal{M} =\\n  \\\\frac{1}{k}\\\\sum_{i=1}^k \\\\Nc(\\\\f{\\\\mu}_i,\\\\f{\\\\Sigma}) ~~\\\\mbox{ where }~~\\\\f{\\\\mu}_i \\\\in (\\\\epsilon\\\\mathbb{Z})^{d} \\\\]\\n\\nNow, we again use the minimum distance estimator as in Section \\\\ref{sec:imp} \\\\cite[Section~6.8]{devroye2012combinatorial}. Recall the set $\\\\ca{A}$ which was defined as\\n\\\\begin{align*}\\n\\\\Ac \\\\equiv \\\\{\\\\{x: \\\\Mc(x) \\\\ge \\\\Mc'(x)\\\\}: \\\\text{ for any two mixtures } \\\\Mc \\\\ne \\\\Mc'\\\\}\\n\\\\end{align*}\\na collection of subsets. We also know (refer to Section \\\\ref{sec:imp}) that given $m$ samples from $\\\\Mc$ and with $\\\\Delta = \\\\sup_{A \\\\in \\\\Ac}|\\\\Pr_{\\\\sim\\\\Mc}(A) - P_m(A)|$, we have\\n  $$\\n  \\\\variation{\\\\hat{\\\\Mc} -\\\\Mc} \\\\le 4\\\\Delta +\\\\frac{3}{m}.\\n  $$\\nwhere $P_m(A)$ is the empirical probability induced by the $m$ samples. We now upper bound the right-hand side of the above inequality.  Via\\nMcDiarmid's inequality and a standard symmetrization argument, $\\\\Delta$ is concentrated around its mean which is a function of $VC(\\\\Ac)$, the VC dimension of the class $\\\\Ac$, see\\n\\\\cite[Section~4.3]{devroye2012combinatorial}:  % we have\\n$$\\n\\\\variation{\\\\hat{\\\\Mc} - \\\\Mc} \\\\leq 4\\\\Delta + O(1/m) \\\\leq 4\\\\avg_{\\\\sim\\\\Mc} \\\\Delta + O(1/\\\\sqrt{m}) \\\\le c \\\\sqrt{\\\\frac{VC(\\\\Ac)}{m}},\\n$$ with high probability, for an absolute constant $c$. Now, for the class $\\\\Ac$ defined above, the VC dimension is given by $VC(\\\\Ac) = O(k)$ and as a result the error of the minimum distance estimator is $O(\\\\sqrt{k/m})$ with high probability. \\n Therefore, as long as $\\\\variation{\\\\hat{\\\\Mc} -\\\\Mc} \\\\le \\\\frac12 \\\\variation{\\\\Mc -\\\\Mc'}$ we will exactly identify the parameters. Hence\\n$$m = k^3\\\\exp\\\\Bigg(O\\\\Big(\\\\Big(\\\\frac{\\\\lambda^{\\\\star}d(\\\\log k)^{1/d}}{\\\\epsilon^{2}}\\\\Big)^{\\\\frac{d}{d+2}}\\\\Big)\\\\Bigg) $$ samples suffice to exactly learn the parameters with high probability.\\n}\\n\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_latex_file_content_new = main_latex_text_wo_comments\n",
        "for import_file in input_file_path_content_dir:\n",
        "  print(import_file)\n",
        "  main_latex_file_content_new = main_latex_file_content_new.replace(import_file, input_file_path_content_dir[import_file] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBpmdakL3ppF",
        "outputId": "cf72c34f-ca21-402d-9b2d-2c4925111f17"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\input{01-intro}\n",
            "\n",
            "\\input{02-complex}\n",
            "\n",
            "\\input{03-moments}\n",
            "\n",
            "\\input{05-appendix}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main_latex_file_content_new"
      ],
      "metadata": {
        "id": "5DDjVfFX56K1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(main_latex_file_content_new)"
      ],
      "metadata": {
        "id": "KzZzM3K06kSF"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}